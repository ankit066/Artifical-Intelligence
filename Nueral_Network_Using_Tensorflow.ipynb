{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nueral Network Using Tensorflow",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIT__IGQh-nF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b7faadb4-8e38-45ff-90bc-1caed7d5dbb3"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF_4uTxC-EQ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "f292e7d8-b393-40e9-f90a-849c6b1834f2"
      },
      "source": [
        "!pip install kaggle\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBpXosPt-I9A",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "ba0e1e71-7b57-4a92-ba63-64f60df9ec1b"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17fce30f-4eee-4987-a4df-9e94ebebfa70\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-17fce30f-4eee-4987-a4df-9e94ebebfa70\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"dixit066\",\"key\":\"1a97d7c958cfe851290b5e0bf103919c\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9feSfiJ-lWU"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI2qJTS3-o_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "344924a7-b32f-4e06-e7b7-54452c94f54d"
      },
      "source": [
        "!kaggle datasets download -d mattcarter865/mines-vs-rocks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading mines-vs-rocks.zip to /content\n",
            "\r  0% 0.00/29.1k [00:00<?, ?B/s]\n",
            "\r100% 29.1k/29.1k [00:00<00:00, 11.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSqrm_xs-0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e251052-638a-475a-cdc5-595f2b7b48a3"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "filename= \"mines-vs-rocks.zip\"\n",
        "with ZipFile(filename,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMm2vXIl_JAo"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from  sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLCLA48yIcjh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "537767dd-f5bc-410c-cdfa-e87e4af1a8dd"
      },
      "source": [
        "file=pd.read_csv(\"/content/sonar.all-data.csv\")\n",
        "file.iloc[:,0:60]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.0200</th>\n",
              "      <th>0.0371</th>\n",
              "      <th>0.0428</th>\n",
              "      <th>0.0207</th>\n",
              "      <th>0.0954</th>\n",
              "      <th>0.0986</th>\n",
              "      <th>0.1539</th>\n",
              "      <th>0.1601</th>\n",
              "      <th>0.3109</th>\n",
              "      <th>0.2111</th>\n",
              "      <th>0.1609</th>\n",
              "      <th>0.1582</th>\n",
              "      <th>0.2238</th>\n",
              "      <th>0.0645</th>\n",
              "      <th>0.0660</th>\n",
              "      <th>0.2273</th>\n",
              "      <th>0.3100</th>\n",
              "      <th>0.2999</th>\n",
              "      <th>0.5078</th>\n",
              "      <th>0.4797</th>\n",
              "      <th>0.5783</th>\n",
              "      <th>0.5071</th>\n",
              "      <th>0.4328</th>\n",
              "      <th>0.5550</th>\n",
              "      <th>0.6711</th>\n",
              "      <th>0.6415</th>\n",
              "      <th>0.7104</th>\n",
              "      <th>0.8080</th>\n",
              "      <th>0.6791</th>\n",
              "      <th>0.3857</th>\n",
              "      <th>0.1307</th>\n",
              "      <th>0.2604</th>\n",
              "      <th>0.5121</th>\n",
              "      <th>0.7547</th>\n",
              "      <th>0.8537</th>\n",
              "      <th>0.8507</th>\n",
              "      <th>0.6692</th>\n",
              "      <th>0.6097</th>\n",
              "      <th>0.4943</th>\n",
              "      <th>0.2744</th>\n",
              "      <th>0.0510</th>\n",
              "      <th>0.2834</th>\n",
              "      <th>0.2825</th>\n",
              "      <th>0.4256</th>\n",
              "      <th>0.2641</th>\n",
              "      <th>0.1386</th>\n",
              "      <th>0.1051</th>\n",
              "      <th>0.1343</th>\n",
              "      <th>0.0383</th>\n",
              "      <th>0.0324</th>\n",
              "      <th>0.0232</th>\n",
              "      <th>0.0027</th>\n",
              "      <th>0.0065</th>\n",
              "      <th>0.0159</th>\n",
              "      <th>0.0072</th>\n",
              "      <th>0.0167</th>\n",
              "      <th>0.0180</th>\n",
              "      <th>0.0084</th>\n",
              "      <th>0.0090</th>\n",
              "      <th>0.0032</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0286</td>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>0.0174</td>\n",
              "      <td>0.0384</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.1201</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.2105</td>\n",
              "      <td>0.3039</td>\n",
              "      <td>0.2988</td>\n",
              "      <td>0.4250</td>\n",
              "      <td>0.6343</td>\n",
              "      <td>0.8198</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9988</td>\n",
              "      <td>0.9508</td>\n",
              "      <td>0.9025</td>\n",
              "      <td>0.7234</td>\n",
              "      <td>0.5122</td>\n",
              "      <td>0.2074</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.5890</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.2043</td>\n",
              "      <td>0.5782</td>\n",
              "      <td>0.5389</td>\n",
              "      <td>0.3750</td>\n",
              "      <td>0.3411</td>\n",
              "      <td>0.5067</td>\n",
              "      <td>0.5580</td>\n",
              "      <td>0.4778</td>\n",
              "      <td>0.3299</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>0.1407</td>\n",
              "      <td>0.2856</td>\n",
              "      <td>0.3807</td>\n",
              "      <td>0.4158</td>\n",
              "      <td>0.4054</td>\n",
              "      <td>0.3296</td>\n",
              "      <td>0.2707</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.0723</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1192</td>\n",
              "      <td>0.1089</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0494</td>\n",
              "      <td>0.0264</td>\n",
              "      <td>0.0081</td>\n",
              "      <td>0.0104</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.0346</td>\n",
              "      <td>0.0168</td>\n",
              "      <td>0.0177</td>\n",
              "      <td>0.0393</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.2328</td>\n",
              "      <td>0.2684</td>\n",
              "      <td>0.3108</td>\n",
              "      <td>0.2933</td>\n",
              "      <td>0.2275</td>\n",
              "      <td>0.0994</td>\n",
              "      <td>0.1801</td>\n",
              "      <td>0.2200</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2034</td>\n",
              "      <td>0.1740</td>\n",
              "      <td>0.4130</td>\n",
              "      <td>0.6879</td>\n",
              "      <td>0.8120</td>\n",
              "      <td>0.8453</td>\n",
              "      <td>0.8919</td>\n",
              "      <td>0.9300</td>\n",
              "      <td>0.9987</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8104</td>\n",
              "      <td>0.6199</td>\n",
              "      <td>0.6041</td>\n",
              "      <td>0.5547</td>\n",
              "      <td>0.4160</td>\n",
              "      <td>0.1472</td>\n",
              "      <td>0.0849</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.0969</td>\n",
              "      <td>0.1411</td>\n",
              "      <td>0.1676</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.1201</td>\n",
              "      <td>0.1036</td>\n",
              "      <td>0.1977</td>\n",
              "      <td>0.1339</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>0.1085</td>\n",
              "      <td>0.1521</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.0858</td>\n",
              "      <td>0.0290</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0098</td>\n",
              "      <td>0.0199</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0115</td>\n",
              "      <td>0.0193</td>\n",
              "      <td>0.0157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.0323</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0298</td>\n",
              "      <td>0.0564</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.0958</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.1018</td>\n",
              "      <td>0.1030</td>\n",
              "      <td>0.2154</td>\n",
              "      <td>0.3085</td>\n",
              "      <td>0.3425</td>\n",
              "      <td>0.2990</td>\n",
              "      <td>0.1402</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>0.1534</td>\n",
              "      <td>0.1901</td>\n",
              "      <td>0.2429</td>\n",
              "      <td>0.2120</td>\n",
              "      <td>0.2395</td>\n",
              "      <td>0.3272</td>\n",
              "      <td>0.5949</td>\n",
              "      <td>0.8302</td>\n",
              "      <td>0.9045</td>\n",
              "      <td>0.9888</td>\n",
              "      <td>0.9912</td>\n",
              "      <td>0.9448</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9092</td>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.7691</td>\n",
              "      <td>0.7117</td>\n",
              "      <td>0.5304</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>0.1297</td>\n",
              "      <td>0.1159</td>\n",
              "      <td>0.1226</td>\n",
              "      <td>0.1768</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.1562</td>\n",
              "      <td>0.0824</td>\n",
              "      <td>0.1149</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>0.1255</td>\n",
              "      <td>0.0647</td>\n",
              "      <td>0.0179</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0093</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.0522</td>\n",
              "      <td>0.0437</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0351</td>\n",
              "      <td>0.1171</td>\n",
              "      <td>0.1257</td>\n",
              "      <td>0.1178</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.2529</td>\n",
              "      <td>0.2716</td>\n",
              "      <td>0.2374</td>\n",
              "      <td>0.1878</td>\n",
              "      <td>0.0983</td>\n",
              "      <td>0.0683</td>\n",
              "      <td>0.1503</td>\n",
              "      <td>0.1723</td>\n",
              "      <td>0.2339</td>\n",
              "      <td>0.1962</td>\n",
              "      <td>0.1395</td>\n",
              "      <td>0.3164</td>\n",
              "      <td>0.5888</td>\n",
              "      <td>0.7631</td>\n",
              "      <td>0.8473</td>\n",
              "      <td>0.9424</td>\n",
              "      <td>0.9986</td>\n",
              "      <td>0.9699</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8630</td>\n",
              "      <td>0.6979</td>\n",
              "      <td>0.7717</td>\n",
              "      <td>0.7305</td>\n",
              "      <td>0.5197</td>\n",
              "      <td>0.1786</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1446</td>\n",
              "      <td>0.1066</td>\n",
              "      <td>0.1440</td>\n",
              "      <td>0.1929</td>\n",
              "      <td>0.0325</td>\n",
              "      <td>0.1490</td>\n",
              "      <td>0.0328</td>\n",
              "      <td>0.0537</td>\n",
              "      <td>0.1309</td>\n",
              "      <td>0.0910</td>\n",
              "      <td>0.0757</td>\n",
              "      <td>0.1059</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0535</td>\n",
              "      <td>0.0235</td>\n",
              "      <td>0.0155</td>\n",
              "      <td>0.0160</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0138</td>\n",
              "      <td>0.0077</td>\n",
              "      <td>0.0031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>0.0303</td>\n",
              "      <td>0.0353</td>\n",
              "      <td>0.0490</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.1354</td>\n",
              "      <td>0.1465</td>\n",
              "      <td>0.1123</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>0.2898</td>\n",
              "      <td>0.2812</td>\n",
              "      <td>0.1578</td>\n",
              "      <td>0.0273</td>\n",
              "      <td>0.0673</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.2070</td>\n",
              "      <td>0.2645</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.5685</td>\n",
              "      <td>0.6990</td>\n",
              "      <td>0.7246</td>\n",
              "      <td>0.7622</td>\n",
              "      <td>0.9242</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9979</td>\n",
              "      <td>0.8297</td>\n",
              "      <td>0.7032</td>\n",
              "      <td>0.7141</td>\n",
              "      <td>0.6893</td>\n",
              "      <td>0.4961</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.0969</td>\n",
              "      <td>0.0776</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.1572</td>\n",
              "      <td>0.1823</td>\n",
              "      <td>0.1349</td>\n",
              "      <td>0.0849</td>\n",
              "      <td>0.0492</td>\n",
              "      <td>0.1367</td>\n",
              "      <td>0.1552</td>\n",
              "      <td>0.1548</td>\n",
              "      <td>0.1319</td>\n",
              "      <td>0.0985</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0489</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0.0260</td>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.0272</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0338</td>\n",
              "      <td>0.0655</td>\n",
              "      <td>0.1400</td>\n",
              "      <td>0.1843</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>0.2720</td>\n",
              "      <td>0.2442</td>\n",
              "      <td>0.1665</td>\n",
              "      <td>0.0336</td>\n",
              "      <td>0.1302</td>\n",
              "      <td>0.1708</td>\n",
              "      <td>0.2177</td>\n",
              "      <td>0.3175</td>\n",
              "      <td>0.3714</td>\n",
              "      <td>0.4552</td>\n",
              "      <td>0.5700</td>\n",
              "      <td>0.7397</td>\n",
              "      <td>0.8062</td>\n",
              "      <td>0.8837</td>\n",
              "      <td>0.9432</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9375</td>\n",
              "      <td>0.7603</td>\n",
              "      <td>0.7123</td>\n",
              "      <td>0.8358</td>\n",
              "      <td>0.7622</td>\n",
              "      <td>0.4567</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.1549</td>\n",
              "      <td>0.1641</td>\n",
              "      <td>0.1869</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1713</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0768</td>\n",
              "      <td>0.0847</td>\n",
              "      <td>0.2076</td>\n",
              "      <td>0.2505</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.1439</td>\n",
              "      <td>0.1470</td>\n",
              "      <td>0.0991</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0181</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.0129</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0115</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>207 rows × 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0.0200  0.0371  0.0428  0.0207  ...  0.0180  0.0084  0.0090  0.0032\n",
              "0    0.0453  0.0523  0.0843  0.0689  ...  0.0140  0.0049  0.0052  0.0044\n",
              "1    0.0262  0.0582  0.1099  0.1083  ...  0.0316  0.0164  0.0095  0.0078\n",
              "2    0.0100  0.0171  0.0623  0.0205  ...  0.0050  0.0044  0.0040  0.0117\n",
              "3    0.0762  0.0666  0.0481  0.0394  ...  0.0072  0.0048  0.0107  0.0094\n",
              "4    0.0286  0.0453  0.0277  0.0174  ...  0.0057  0.0027  0.0051  0.0062\n",
              "..      ...     ...     ...     ...  ...     ...     ...     ...     ...\n",
              "202  0.0187  0.0346  0.0168  0.0177  ...  0.0065  0.0115  0.0193  0.0157\n",
              "203  0.0323  0.0101  0.0298  0.0564  ...  0.0034  0.0032  0.0062  0.0067\n",
              "204  0.0522  0.0437  0.0180  0.0292  ...  0.0140  0.0138  0.0077  0.0031\n",
              "205  0.0303  0.0353  0.0490  0.0608  ...  0.0034  0.0079  0.0036  0.0048\n",
              "206  0.0260  0.0363  0.0136  0.0272  ...  0.0040  0.0036  0.0061  0.0115\n",
              "\n",
              "[207 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXsLek5sOKUQ"
      },
      "source": [
        "y_tr=[]\n",
        "y_t=file.iloc[:,60]\n",
        "y_t=np.asarray(y_t)\n",
        "for i in range(len(y_t)):\n",
        "  if y_t[i]==\"R\":\n",
        "    y_tr.append(\"0\")\n",
        "  else:\n",
        "    y_tr.append(\"1\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvTrMMyFO1bX"
      },
      "source": [
        "y_tr=shuffle(y_tr,random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp42G2j1PBjl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "687fe3a8-5ced-44e3-e53d-ca146cf2c98c"
      },
      "source": [
        "y_tri,y_tesi=train_test_split(y_tr,test_size=0.20,random_state=415)\n",
        "y_tri=np.asarray(y_tri)\n",
        "y_tesi=np.asarray(y_tesi)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDUpoTHVAlBQ"
      },
      "source": [
        "def read_dataset():\n",
        "  file=pd.read_csv(\"/content/sonar.all-data.csv\")\n",
        "  y=file.iloc[:,60]\n",
        "  X=file.iloc[:,0:60]\n",
        "\n",
        "  encoder=LabelEncoder()\n",
        "  encoder.fit(y)\n",
        "  y=encoder.transform(y)\n",
        "  Y=one_hot_encode(y)\n",
        "  print(X.shape)\n",
        "  return X,Y\n",
        "\n",
        "\n",
        "def one_hot_encode(labels):\n",
        "  n_labels=len(labels)\n",
        "  n_unique_labels=len(np.unique(labels))\n",
        "  one_hot_encode=np.zeros((n_labels,n_unique_labels))\n",
        "  one_hot_encode[np.arange(n_labels),labels]=1\n",
        "  return one_hot_encode\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVEaR63JO6m2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20713843-0f57-47ed-ec13-ada3a0e51e67"
      },
      "source": [
        "X,Y=read_dataset()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(207, 60)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Or686xrO9YJ"
      },
      "source": [
        "X,Y=shuffle(X,Y,random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjkJMBYpPNCh"
      },
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.20,random_state=415)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26ESKrsXP6yU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "952bd173-adbc-46f6-d85c-5e825f21a8af"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(165, 60)\n",
            "(165, 2)\n",
            "(42, 60)\n",
            "(42, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZaxLcy9QQN6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1130969d-8564-464a-e2cb-e277e4fcac10"
      },
      "source": [
        "learning_rate=0.3\n",
        "training_epochs=1000\n",
        "cost_history=np.empty(shape=[1],dtype=float)\n",
        "n_dim=X.shape[1]\n",
        "print(n_dim)\n",
        "n_classes=2\n",
        "model_path=\"/content/sample_data\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_HXPDbRRY_p"
      },
      "source": [
        "n_hidden1=60\n",
        "n_hidden2=60\n",
        "n_hidden3=60\n",
        "n_hidden4=60\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOHYhOhCS7u5"
      },
      "source": [
        "x=tf.placeholder(tf.float32,[None,n_dim])\n",
        "W=tf.Variable(tf.zeros([n_dim,n_classes]))\n",
        "b=tf.Variable(tf.zeros([n_classes]))\n",
        "y_=tf.placeholder(tf.float32,[None,n_classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaGyPZOQrOvP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaJJzgTZxGZU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTQjAcIeUouY"
      },
      "source": [
        "def multilayer_precpetron(x,wieghts,baises):\n",
        "  layer_1=tf.add(tf.matmul(x,wieghts[\"w1\"]),baises[\"b1\"])\n",
        "  layer_1=tf.nn.sigmoid(layer_1)\n",
        "  layer_2=tf.add(tf.matmul(layer_1,wieghts[\"w2\"]),baises[\"b2\"])\n",
        "  layer_2=tf.nn.sigmoid(layer_2)\n",
        "  layer_3=tf.add(tf.matmul(layer_2,wieghts[\"w3\"]),baises[\"b3\"])\n",
        "  layer_3=tf.nn.sigmoid(layer_3)\n",
        "  layer_4=tf.add(tf.matmul(layer_3,wieghts[\"w4\"]),baises[\"b4\"])\n",
        "  layer_4=tf.nn.relu(layer_4)\n",
        "  out_layer=(tf.add(tf.matmul(layer_4,wieghts[\"out\"]),baises[\"out\"]))\n",
        "  return out_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve6b4UTAW9l1"
      },
      "source": [
        "wieghts={\"w1\":tf.Variable(tf.truncated_normal([n_dim,n_hidden1])),\n",
        "         \"w2\":tf.Variable(tf.truncated_normal([n_hidden1,n_hidden2])),\n",
        "         \"w3\":tf.Variable(tf.truncated_normal([n_hidden2,n_hidden3])),\n",
        "         \"w4\":tf.Variable(tf.truncated_normal([n_hidden3,n_hidden4])),\n",
        "\n",
        "         \"out\":tf.Variable(tf.truncated_normal([n_hidden4,n_classes]))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuLpZqWGZB0N"
      },
      "source": [
        "baises={\"b1\":tf.Variable(tf.truncated_normal([n_hidden1])),\n",
        "        \"b2\":tf.Variable(tf.truncated_normal([n_hidden2])),\n",
        "        \"b3\":tf.Variable(tf.truncated_normal([n_hidden3])),\n",
        "        \"b4\":tf.Variable(tf.truncated_normal([n_hidden4])),\n",
        "        \"out\":tf.Variable(tf.truncated_normal([n_classes]))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_13YIbw3x0if"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4-EUqEaru3n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4c0dd0d-295b-49ca-b017-11941f7f379c"
      },
      "source": [
        "multilayer_precpetron(x,wieghts,baises)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Add_4:0' shape=(?, 2) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvMwRZm2Z4LT"
      },
      "source": [
        "init=tf.global_variables_initializer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGXQDdtLaIQs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "95e3896c-3608-446b-d086-2e307c4f9255"
      },
      "source": [
        "y=multilayer_precpetron(x,wieghts,baises)\n",
        "print(y)\n",
        "print(y_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Add_9:0\", shape=(?, 2), dtype=float32)\n",
            "Tensor(\"Placeholder_1:0\", shape=(?, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKpUWYVpqjc1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU4VDXwtakDf"
      },
      "source": [
        "cost_function=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y,labels=y_))\n",
        "training_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhdQFeVob5s9"
      },
      "source": [
        "sess=tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmnBmlAGuY5l"
      },
      "source": [
        "mse_history=[]\n",
        "accuracy_history=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFkrOdzm1BcL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01bef2af-7169-40c1-b201-a00f6486d4d8"
      },
      "source": [
        "for epochs in range(training_epochs):\n",
        "  sess.run(training_step, feed_dict={x:X_train, y_:Y_train})\n",
        "  cost = sess.run(cost_function,feed_dict={x:X_train, y_:Y_train})\n",
        "  cost_history = np.append(cost_history, cost)\n",
        "  correct_prediction = tf.equal(tf.argmax((y),1),tf.argmax((y_),1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  #print(\"Accuracy: \", (sess.run(accuracy, feed_dict={x:X_test, y_:Y_test})))\n",
        "  pred_y = sess.run(y,feed_dict={x:X_test} )\n",
        "  mse = tf.reduce_mean(tf.square(pred_y - Y_test))\n",
        "  mse_ = sess.run(mse)\n",
        "  mse_history.append(mse_)\n",
        "\n",
        "  accuracy = (sess.run(accuracy,feed_dict={x:X_train, y_:Y_train}))\n",
        "  accuracy_history.append(accuracy)\n",
        "  print('epoch: ', epochs,' - ', 'cost: ', cost, \" - MSE: \", mse_, \"- Train Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0  -  cost:  0.23328923  - MSE:  11.157315867836974 - Train Accuracy:  0.92121214\n",
            "epoch:  1  -  cost:  0.23423843  - MSE:  10.696907435870386 - Train Accuracy:  0.90909094\n",
            "epoch:  2  -  cost:  0.2404449  - MSE:  11.001703812955137 - Train Accuracy:  0.8848485\n",
            "epoch:  3  -  cost:  0.23596957  - MSE:  10.932904975059458 - Train Accuracy:  0.90909094\n",
            "epoch:  4  -  cost:  0.25844106  - MSE:  11.042656829347091 - Train Accuracy:  0.8606061\n",
            "epoch:  5  -  cost:  0.26705515  - MSE:  9.957713530787348 - Train Accuracy:  0.8727273\n",
            "epoch:  6  -  cost:  0.25119078  - MSE:  11.209442692192987 - Train Accuracy:  0.8666667\n",
            "epoch:  7  -  cost:  0.26505995  - MSE:  11.637660669824605 - Train Accuracy:  0.8787879\n",
            "epoch:  8  -  cost:  0.30859607  - MSE:  11.312675837431183 - Train Accuracy:  0.8242424\n",
            "epoch:  9  -  cost:  0.35279727  - MSE:  10.129336921114506 - Train Accuracy:  0.8121212\n",
            "epoch:  10  -  cost:  0.27317336  - MSE:  10.23392416485279 - Train Accuracy:  0.8363636\n",
            "epoch:  11  -  cost:  0.26519874  - MSE:  9.161005440897899 - Train Accuracy:  0.9030303\n",
            "epoch:  12  -  cost:  0.24033687  - MSE:  10.45589380772381 - Train Accuracy:  0.8909091\n",
            "epoch:  13  -  cost:  0.22104067  - MSE:  10.50279748553166 - Train Accuracy:  0.93333334\n",
            "epoch:  14  -  cost:  0.19848645  - MSE:  11.221495765736407 - Train Accuracy:  0.92727274\n",
            "epoch:  15  -  cost:  0.18508755  - MSE:  11.995703718123904 - Train Accuracy:  0.95151514\n",
            "epoch:  16  -  cost:  0.18576841  - MSE:  12.214578307502904 - Train Accuracy:  0.93333334\n",
            "epoch:  17  -  cost:  0.20136286  - MSE:  13.47798983142838 - Train Accuracy:  0.92727274\n",
            "epoch:  18  -  cost:  0.2628529  - MSE:  13.049306225797555 - Train Accuracy:  0.8545455\n",
            "epoch:  19  -  cost:  0.33148694  - MSE:  12.637057138948709 - Train Accuracy:  0.8242424\n",
            "epoch:  20  -  cost:  0.45317355  - MSE:  12.437359578745186 - Train Accuracy:  0.75757575\n",
            "epoch:  21  -  cost:  0.545441  - MSE:  8.904341066992918 - Train Accuracy:  0.6424242\n",
            "epoch:  22  -  cost:  0.25409216  - MSE:  8.417825583092238 - Train Accuracy:  0.9030303\n",
            "epoch:  23  -  cost:  0.20791529  - MSE:  8.072475780819852 - Train Accuracy:  0.95151514\n",
            "epoch:  24  -  cost:  0.18489797  - MSE:  9.751071783284189 - Train Accuracy:  0.94545454\n",
            "epoch:  25  -  cost:  0.1778334  - MSE:  11.661392153728812 - Train Accuracy:  0.94545454\n",
            "epoch:  26  -  cost:  0.19250244  - MSE:  12.024656136254949 - Train Accuracy:  0.92727274\n",
            "epoch:  27  -  cost:  0.22124021  - MSE:  13.461832549451582 - Train Accuracy:  0.91515154\n",
            "epoch:  28  -  cost:  0.31970415  - MSE:  12.863640320865821 - Train Accuracy:  0.8242424\n",
            "epoch:  29  -  cost:  0.34223413  - MSE:  11.350130880600672 - Train Accuracy:  0.8242424\n",
            "epoch:  30  -  cost:  0.26344496  - MSE:  10.422820039405348 - Train Accuracy:  0.8727273\n",
            "epoch:  31  -  cost:  0.21506692  - MSE:  9.322525864399257 - Train Accuracy:  0.94545454\n",
            "epoch:  32  -  cost:  0.1867975  - MSE:  10.485667499011726 - Train Accuracy:  0.93939394\n",
            "epoch:  33  -  cost:  0.16654848  - MSE:  11.972295442869532 - Train Accuracy:  0.95151514\n",
            "epoch:  34  -  cost:  0.15950948  - MSE:  13.358521381297132 - Train Accuracy:  0.94545454\n",
            "epoch:  35  -  cost:  0.16666843  - MSE:  14.593950635071625 - Train Accuracy:  0.95151514\n",
            "epoch:  36  -  cost:  0.2087179  - MSE:  13.91260195204072 - Train Accuracy:  0.92121214\n",
            "epoch:  37  -  cost:  0.2526374  - MSE:  14.334455600573381 - Train Accuracy:  0.8727273\n",
            "epoch:  38  -  cost:  0.36624548  - MSE:  12.17547542098102 - Train Accuracy:  0.7878788\n",
            "epoch:  39  -  cost:  0.4101467  - MSE:  10.620327817026208 - Train Accuracy:  0.76363635\n",
            "epoch:  40  -  cost:  0.22005749  - MSE:  10.486007575188731 - Train Accuracy:  0.91515154\n",
            "epoch:  41  -  cost:  0.20082425  - MSE:  10.219063324610225 - Train Accuracy:  0.93939394\n",
            "epoch:  42  -  cost:  0.18654823  - MSE:  11.277870765948686 - Train Accuracy:  0.93333334\n",
            "epoch:  43  -  cost:  0.17482425  - MSE:  12.774383607933302 - Train Accuracy:  0.95151514\n",
            "epoch:  44  -  cost:  0.18982108  - MSE:  13.360694517169536 - Train Accuracy:  0.92121214\n",
            "epoch:  45  -  cost:  0.2075279  - MSE:  14.093021968951781 - Train Accuracy:  0.92121214\n",
            "epoch:  46  -  cost:  0.26460734  - MSE:  13.223578991560467 - Train Accuracy:  0.8606061\n",
            "epoch:  47  -  cost:  0.24857457  - MSE:  11.601362911963072 - Train Accuracy:  0.9030303\n",
            "epoch:  48  -  cost:  0.20932087  - MSE:  12.230062597744755 - Train Accuracy:  0.9030303\n",
            "epoch:  49  -  cost:  0.17940667  - MSE:  12.840420992997176 - Train Accuracy:  0.94545454\n",
            "epoch:  50  -  cost:  0.17307773  - MSE:  13.618143333689066 - Train Accuracy:  0.94545454\n",
            "epoch:  51  -  cost:  0.16647857  - MSE:  14.85200679101474 - Train Accuracy:  0.95151514\n",
            "epoch:  52  -  cost:  0.18548757  - MSE:  14.865080178085403 - Train Accuracy:  0.93333334\n",
            "epoch:  53  -  cost:  0.21817663  - MSE:  16.002109741743567 - Train Accuracy:  0.90909094\n",
            "epoch:  54  -  cost:  0.32129365  - MSE:  13.862786442207204 - Train Accuracy:  0.8060606\n",
            "epoch:  55  -  cost:  0.36611295  - MSE:  12.84031134812287 - Train Accuracy:  0.8\n",
            "epoch:  56  -  cost:  0.25795025  - MSE:  11.470350257723675 - Train Accuracy:  0.8666667\n",
            "epoch:  57  -  cost:  0.23031557  - MSE:  9.656850609534226 - Train Accuracy:  0.93333334\n",
            "epoch:  58  -  cost:  0.19012468  - MSE:  11.311211883721658 - Train Accuracy:  0.93939394\n",
            "epoch:  59  -  cost:  0.16327025  - MSE:  12.397719800439242 - Train Accuracy:  0.95151514\n",
            "epoch:  60  -  cost:  0.15123561  - MSE:  14.547837884681108 - Train Accuracy:  0.95757574\n",
            "epoch:  61  -  cost:  0.15378724  - MSE:  16.06499858849067 - Train Accuracy:  0.95151514\n",
            "epoch:  62  -  cost:  0.17774475  - MSE:  15.714009278663779 - Train Accuracy:  0.93939394\n",
            "epoch:  63  -  cost:  0.21603805  - MSE:  16.92322355582902 - Train Accuracy:  0.90909094\n",
            "epoch:  64  -  cost:  0.33597663  - MSE:  14.436090731566846 - Train Accuracy:  0.8\n",
            "epoch:  65  -  cost:  0.40978467  - MSE:  13.77697875739885 - Train Accuracy:  0.76363635\n",
            "epoch:  66  -  cost:  0.28298628  - MSE:  11.245536009613375 - Train Accuracy:  0.8606061\n",
            "epoch:  67  -  cost:  0.23199971  - MSE:  8.922169664433273 - Train Accuracy:  0.93333334\n",
            "epoch:  68  -  cost:  0.17739688  - MSE:  11.320752873905192 - Train Accuracy:  0.94545454\n",
            "epoch:  69  -  cost:  0.15519626  - MSE:  13.529498286635052 - Train Accuracy:  0.95757574\n",
            "epoch:  70  -  cost:  0.15249471  - MSE:  14.806322676286852 - Train Accuracy:  0.95151514\n",
            "epoch:  71  -  cost:  0.15455781  - MSE:  16.05305788631126 - Train Accuracy:  0.95151514\n",
            "epoch:  72  -  cost:  0.19207615  - MSE:  15.866049999838713 - Train Accuracy:  0.92121214\n",
            "epoch:  73  -  cost:  0.23350532  - MSE:  16.90950614894816 - Train Accuracy:  0.8969697\n",
            "epoch:  74  -  cost:  0.32346773  - MSE:  13.647772713534664 - Train Accuracy:  0.8060606\n",
            "epoch:  75  -  cost:  0.34522203  - MSE:  12.278058648736138 - Train Accuracy:  0.830303\n",
            "epoch:  76  -  cost:  0.19940537  - MSE:  12.325677331861574 - Train Accuracy:  0.91515154\n",
            "epoch:  77  -  cost:  0.17011409  - MSE:  12.207231997481326 - Train Accuracy:  0.95757574\n",
            "epoch:  78  -  cost:  0.15623544  - MSE:  13.482089573137776 - Train Accuracy:  0.95757574\n",
            "epoch:  79  -  cost:  0.14014228  - MSE:  15.285642269775806 - Train Accuracy:  0.95757574\n",
            "epoch:  80  -  cost:  0.13309994  - MSE:  16.216510431292292 - Train Accuracy:  0.969697\n",
            "epoch:  81  -  cost:  0.12941845  - MSE:  17.49020733012527 - Train Accuracy:  0.96363634\n",
            "epoch:  82  -  cost:  0.1337092  - MSE:  17.587083247231014 - Train Accuracy:  0.95757574\n",
            "epoch:  83  -  cost:  0.14298333  - MSE:  18.740949637728775 - Train Accuracy:  0.95151514\n",
            "epoch:  84  -  cost:  0.18664414  - MSE:  17.510197052726614 - Train Accuracy:  0.92727274\n",
            "epoch:  85  -  cost:  0.26656407  - MSE:  19.34190212684873 - Train Accuracy:  0.8545455\n",
            "epoch:  86  -  cost:  0.5105974  - MSE:  15.932543425694169 - Train Accuracy:  0.73939395\n",
            "epoch:  87  -  cost:  0.71867245  - MSE:  12.816020449622913 - Train Accuracy:  0.56969696\n",
            "epoch:  88  -  cost:  0.29144695  - MSE:  10.77533706120642 - Train Accuracy:  0.8787879\n",
            "epoch:  89  -  cost:  0.2546308  - MSE:  8.67908441913378 - Train Accuracy:  0.92727274\n",
            "epoch:  90  -  cost:  0.1959903  - MSE:  10.008090700396801 - Train Accuracy:  0.92121214\n",
            "epoch:  91  -  cost:  0.1665436  - MSE:  11.660148853917566 - Train Accuracy:  0.95757574\n",
            "epoch:  92  -  cost:  0.15605009  - MSE:  12.910702495765516 - Train Accuracy:  0.94545454\n",
            "epoch:  93  -  cost:  0.14577715  - MSE:  14.30741261706461 - Train Accuracy:  0.95757574\n",
            "epoch:  94  -  cost:  0.15329625  - MSE:  14.862644350743093 - Train Accuracy:  0.94545454\n",
            "epoch:  95  -  cost:  0.16356038  - MSE:  15.78643082953732 - Train Accuracy:  0.95757574\n",
            "epoch:  96  -  cost:  0.23174696  - MSE:  16.20140319442313 - Train Accuracy:  0.8848485\n",
            "epoch:  97  -  cost:  0.34108302  - MSE:  16.910073228417314 - Train Accuracy:  0.8\n",
            "epoch:  98  -  cost:  0.5269175  - MSE:  15.697655692106366 - Train Accuracy:  0.7151515\n",
            "epoch:  99  -  cost:  0.5690458  - MSE:  8.274509712401683 - Train Accuracy:  0.6242424\n",
            "epoch:  100  -  cost:  0.23620479  - MSE:  9.242027426258874 - Train Accuracy:  0.8969697\n",
            "epoch:  101  -  cost:  0.18512404  - MSE:  10.03923727779354 - Train Accuracy:  0.95151514\n",
            "epoch:  102  -  cost:  0.15294416  - MSE:  12.65249449534316 - Train Accuracy:  0.95151514\n",
            "epoch:  103  -  cost:  0.13568684  - MSE:  15.084690895017284 - Train Accuracy:  0.96363634\n",
            "epoch:  104  -  cost:  0.12816755  - MSE:  16.381088566051083 - Train Accuracy:  0.96363634\n",
            "epoch:  105  -  cost:  0.12373042  - MSE:  17.319145134651993 - Train Accuracy:  0.969697\n",
            "epoch:  106  -  cost:  0.1199551  - MSE:  18.235227315115214 - Train Accuracy:  0.9757576\n",
            "epoch:  107  -  cost:  0.11740446  - MSE:  18.895794076985236 - Train Accuracy:  0.969697\n",
            "epoch:  108  -  cost:  0.11531536  - MSE:  19.413506093288305 - Train Accuracy:  0.9818182\n",
            "epoch:  109  -  cost:  0.11319194  - MSE:  20.110620186025773 - Train Accuracy:  0.969697\n",
            "epoch:  110  -  cost:  0.11507802  - MSE:  19.82532091336536 - Train Accuracy:  0.969697\n",
            "epoch:  111  -  cost:  0.121881574  - MSE:  21.107508139602054 - Train Accuracy:  0.969697\n",
            "epoch:  112  -  cost:  0.15201993  - MSE:  19.68611098073957 - Train Accuracy:  0.94545454\n",
            "epoch:  113  -  cost:  0.2072081  - MSE:  21.782857572191535 - Train Accuracy:  0.91515154\n",
            "epoch:  114  -  cost:  0.42960593  - MSE:  18.063445604848933 - Train Accuracy:  0.76363635\n",
            "epoch:  115  -  cost:  0.5521765  - MSE:  15.442786600057792 - Train Accuracy:  0.6666667\n",
            "epoch:  116  -  cost:  0.35351992  - MSE:  10.120483231140643 - Train Accuracy:  0.8363636\n",
            "epoch:  117  -  cost:  0.24065314  - MSE:  6.387680977092109 - Train Accuracy:  0.93333334\n",
            "epoch:  118  -  cost:  0.16433525  - MSE:  10.750704970366266 - Train Accuracy:  0.95757574\n",
            "epoch:  119  -  cost:  0.14660354  - MSE:  13.372017648836975 - Train Accuracy:  0.95757574\n",
            "epoch:  120  -  cost:  0.13239157  - MSE:  15.40799817610521 - Train Accuracy:  0.95757574\n",
            "epoch:  121  -  cost:  0.12229894  - MSE:  17.242585760540692 - Train Accuracy:  0.96363634\n",
            "epoch:  122  -  cost:  0.11609334  - MSE:  18.507006177357116 - Train Accuracy:  0.9757576\n",
            "epoch:  123  -  cost:  0.11190467  - MSE:  19.23352630636445 - Train Accuracy:  0.969697\n",
            "epoch:  124  -  cost:  0.1085064  - MSE:  20.13310661660445 - Train Accuracy:  0.9818182\n",
            "epoch:  125  -  cost:  0.10607667  - MSE:  20.552305414660715 - Train Accuracy:  0.969697\n",
            "epoch:  126  -  cost:  0.10383352  - MSE:  21.302412053738554 - Train Accuracy:  0.9818182\n",
            "epoch:  127  -  cost:  0.103862844  - MSE:  21.914014993362347 - Train Accuracy:  0.969697\n",
            "epoch:  128  -  cost:  0.11258318  - MSE:  21.758740575346685 - Train Accuracy:  0.96363634\n",
            "epoch:  129  -  cost:  0.14252256  - MSE:  23.2197050886153 - Train Accuracy:  0.95757574\n",
            "epoch:  130  -  cost:  0.28826755  - MSE:  21.757709892131558 - Train Accuracy:  0.8484849\n",
            "epoch:  131  -  cost:  0.5572865  - MSE:  25.567197674322166 - Train Accuracy:  0.72727275\n",
            "epoch:  132  -  cost:  1.0580571  - MSE:  19.74721770790696 - Train Accuracy:  0.56363636\n",
            "epoch:  133  -  cost:  0.8659153  - MSE:  3.5320463887546887 - Train Accuracy:  0.5090909\n",
            "epoch:  134  -  cost:  0.38514343  - MSE:  5.125561235992999 - Train Accuracy:  0.830303\n",
            "epoch:  135  -  cost:  0.35430747  - MSE:  3.7159489007833537 - Train Accuracy:  0.8424242\n",
            "epoch:  136  -  cost:  0.3178972  - MSE:  6.237634017162631 - Train Accuracy:  0.8424242\n",
            "epoch:  137  -  cost:  0.28735176  - MSE:  6.83074267390178 - Train Accuracy:  0.8727273\n",
            "epoch:  138  -  cost:  0.250574  - MSE:  9.817758059205625 - Train Accuracy:  0.90909094\n",
            "epoch:  139  -  cost:  0.2274077  - MSE:  11.888831911046877 - Train Accuracy:  0.92727274\n",
            "epoch:  140  -  cost:  0.20966913  - MSE:  14.12536640342083 - Train Accuracy:  0.93939394\n",
            "epoch:  141  -  cost:  0.20112927  - MSE:  15.380034104473044 - Train Accuracy:  0.93333334\n",
            "epoch:  142  -  cost:  0.19780475  - MSE:  16.094609115036196 - Train Accuracy:  0.92727274\n",
            "epoch:  143  -  cost:  0.21477303  - MSE:  17.795520982020367 - Train Accuracy:  0.92727274\n",
            "epoch:  144  -  cost:  0.29823405  - MSE:  16.933355915351758 - Train Accuracy:  0.8363636\n",
            "epoch:  145  -  cost:  0.47684082  - MSE:  19.011096328214315 - Train Accuracy:  0.73939395\n",
            "epoch:  146  -  cost:  0.614991  - MSE:  15.307717659309008 - Train Accuracy:  0.75151515\n",
            "epoch:  147  -  cost:  0.6029161  - MSE:  4.306661435182212 - Train Accuracy:  0.6060606\n",
            "epoch:  148  -  cost:  0.27134752  - MSE:  5.182084077072003 - Train Accuracy:  0.9030303\n",
            "epoch:  149  -  cost:  0.23578116  - MSE:  6.662281455579814 - Train Accuracy:  0.92727274\n",
            "epoch:  150  -  cost:  0.21061996  - MSE:  8.489328338970116 - Train Accuracy:  0.93939394\n",
            "epoch:  151  -  cost:  0.19117776  - MSE:  9.706451167168169 - Train Accuracy:  0.93939394\n",
            "epoch:  152  -  cost:  0.17314038  - MSE:  10.821847914006268 - Train Accuracy:  0.94545454\n",
            "epoch:  153  -  cost:  0.15930684  - MSE:  11.240681190703802 - Train Accuracy:  0.94545454\n",
            "epoch:  154  -  cost:  0.14859939  - MSE:  12.07883841528874 - Train Accuracy:  0.95151514\n",
            "epoch:  155  -  cost:  0.14769587  - MSE:  12.50316348398654 - Train Accuracy:  0.95757574\n",
            "epoch:  156  -  cost:  0.15461414  - MSE:  13.855507605682234 - Train Accuracy:  0.95151514\n",
            "epoch:  157  -  cost:  0.1701802  - MSE:  14.076779755967298 - Train Accuracy:  0.92727274\n",
            "epoch:  158  -  cost:  0.1760634  - MSE:  15.285454451502325 - Train Accuracy:  0.93939394\n",
            "epoch:  159  -  cost:  0.20235203  - MSE:  15.527916599743413 - Train Accuracy:  0.9030303\n",
            "epoch:  160  -  cost:  0.19098769  - MSE:  15.563080861078427 - Train Accuracy:  0.93333334\n",
            "epoch:  161  -  cost:  0.17573348  - MSE:  14.260583217369875 - Train Accuracy:  0.92727274\n",
            "epoch:  162  -  cost:  0.14167002  - MSE:  14.293399625680753 - Train Accuracy:  0.969697\n",
            "epoch:  163  -  cost:  0.12639354  - MSE:  15.800546856951494 - Train Accuracy:  0.96363634\n",
            "epoch:  164  -  cost:  0.12101985  - MSE:  17.91306904471114 - Train Accuracy:  0.96363634\n",
            "epoch:  165  -  cost:  0.12077026  - MSE:  17.97807782497138 - Train Accuracy:  0.969697\n",
            "epoch:  166  -  cost:  0.12747523  - MSE:  19.490256617751314 - Train Accuracy:  0.96363634\n",
            "epoch:  167  -  cost:  0.14497226  - MSE:  18.947891396412984 - Train Accuracy:  0.95151514\n",
            "epoch:  168  -  cost:  0.16408047  - MSE:  20.03794471307122 - Train Accuracy:  0.94545454\n",
            "epoch:  169  -  cost:  0.21491972  - MSE:  19.033227134652606 - Train Accuracy:  0.8909091\n",
            "epoch:  170  -  cost:  0.20926772  - MSE:  18.124767358438802 - Train Accuracy:  0.91515154\n",
            "epoch:  171  -  cost:  0.17219405  - MSE:  16.194377543803306 - Train Accuracy:  0.92727274\n",
            "epoch:  172  -  cost:  0.13729315  - MSE:  15.813491854999906 - Train Accuracy:  0.969697\n",
            "epoch:  173  -  cost:  0.10930147  - MSE:  18.52938873301765 - Train Accuracy:  0.9757576\n",
            "epoch:  174  -  cost:  0.09832686  - MSE:  21.24591292844616 - Train Accuracy:  0.969697\n",
            "epoch:  175  -  cost:  0.09298259  - MSE:  22.445039838923112 - Train Accuracy:  0.9818182\n",
            "epoch:  176  -  cost:  0.091933675  - MSE:  24.100385370685984 - Train Accuracy:  0.9757576\n",
            "epoch:  177  -  cost:  0.091664895  - MSE:  23.90729068963167 - Train Accuracy:  0.9818182\n",
            "epoch:  178  -  cost:  0.09310291  - MSE:  25.51829400086753 - Train Accuracy:  0.9757576\n",
            "epoch:  179  -  cost:  0.10015563  - MSE:  25.06640361290595 - Train Accuracy:  0.9757576\n",
            "epoch:  180  -  cost:  0.11576992  - MSE:  27.018953797658305 - Train Accuracy:  0.96363634\n",
            "epoch:  181  -  cost:  0.19074984  - MSE:  25.208440233954565 - Train Accuracy:  0.8969697\n",
            "epoch:  182  -  cost:  0.3552429  - MSE:  29.048259634776347 - Train Accuracy:  0.8242424\n",
            "epoch:  183  -  cost:  0.8848615  - MSE:  24.708840168839977 - Train Accuracy:  0.6606061\n",
            "epoch:  184  -  cost:  0.92913353  - MSE:  9.536978197567155 - Train Accuracy:  0.5030303\n",
            "epoch:  185  -  cost:  0.32192972  - MSE:  7.85830643404988 - Train Accuracy:  0.8424242\n",
            "epoch:  186  -  cost:  0.24384594  - MSE:  12.101636869834373 - Train Accuracy:  0.91515154\n",
            "epoch:  187  -  cost:  0.19978823  - MSE:  15.681723643949047 - Train Accuracy:  0.93939394\n",
            "epoch:  188  -  cost:  0.17734128  - MSE:  17.998519696097087 - Train Accuracy:  0.94545454\n",
            "epoch:  189  -  cost:  0.1647189  - MSE:  19.44193390418477 - Train Accuracy:  0.95757574\n",
            "epoch:  190  -  cost:  0.15473337  - MSE:  20.79497123452886 - Train Accuracy:  0.95151514\n",
            "epoch:  191  -  cost:  0.14910412  - MSE:  20.329774003612833 - Train Accuracy:  0.96363634\n",
            "epoch:  192  -  cost:  0.14445375  - MSE:  22.39187766595869 - Train Accuracy:  0.95151514\n",
            "epoch:  193  -  cost:  0.14858323  - MSE:  21.68095044573234 - Train Accuracy:  0.95151514\n",
            "epoch:  194  -  cost:  0.15638682  - MSE:  23.50106858545818 - Train Accuracy:  0.95757574\n",
            "epoch:  195  -  cost:  0.1913811  - MSE:  19.856127261336567 - Train Accuracy:  0.91515154\n",
            "epoch:  196  -  cost:  0.19388334  - MSE:  21.17152550317563 - Train Accuracy:  0.91515154\n",
            "epoch:  197  -  cost:  0.21679926  - MSE:  17.52824700027336 - Train Accuracy:  0.8848485\n",
            "epoch:  198  -  cost:  0.17932136  - MSE:  18.417352635042693 - Train Accuracy:  0.93939394\n",
            "epoch:  199  -  cost:  0.11837613  - MSE:  17.43807503277893 - Train Accuracy:  0.9757576\n",
            "epoch:  200  -  cost:  0.10508719  - MSE:  20.685608166829443 - Train Accuracy:  0.96363634\n",
            "epoch:  201  -  cost:  0.09204626  - MSE:  21.929589409842386 - Train Accuracy:  0.9818182\n",
            "epoch:  202  -  cost:  0.08775194  - MSE:  23.93402767241644 - Train Accuracy:  0.9818182\n",
            "epoch:  203  -  cost:  0.0868405  - MSE:  24.11956404021423 - Train Accuracy:  0.9818182\n",
            "epoch:  204  -  cost:  0.090915106  - MSE:  26.853942721246078 - Train Accuracy:  0.9818182\n",
            "epoch:  205  -  cost:  0.106087364  - MSE:  24.511379301410162 - Train Accuracy:  0.969697\n",
            "epoch:  206  -  cost:  0.12102251  - MSE:  28.159034226198976 - Train Accuracy:  0.96363634\n",
            "epoch:  207  -  cost:  0.21125735  - MSE:  23.49365715986189 - Train Accuracy:  0.8848485\n",
            "epoch:  208  -  cost:  0.31610745  - MSE:  28.127990864711396 - Train Accuracy:  0.8242424\n",
            "epoch:  209  -  cost:  0.65368044  - MSE:  19.88033790840536 - Train Accuracy:  0.7030303\n",
            "epoch:  210  -  cost:  0.5820384  - MSE:  8.262382852069782 - Train Accuracy:  0.6424242\n",
            "epoch:  211  -  cost:  0.17824624  - MSE:  9.838428709075856 - Train Accuracy:  0.96363634\n",
            "epoch:  212  -  cost:  0.13565396  - MSE:  13.927956362865931 - Train Accuracy:  0.9757576\n",
            "epoch:  213  -  cost:  0.11523781  - MSE:  17.408695341751073 - Train Accuracy:  0.96363634\n",
            "epoch:  214  -  cost:  0.100429766  - MSE:  20.31048579540009 - Train Accuracy:  0.9818182\n",
            "epoch:  215  -  cost:  0.094409004  - MSE:  22.371377605313494 - Train Accuracy:  0.9818182\n",
            "epoch:  216  -  cost:  0.08950585  - MSE:  23.47396356921552 - Train Accuracy:  0.9818182\n",
            "epoch:  217  -  cost:  0.086793974  - MSE:  25.857920602172346 - Train Accuracy:  0.9818182\n",
            "epoch:  218  -  cost:  0.088687584  - MSE:  25.368642958573353 - Train Accuracy:  0.9878788\n",
            "epoch:  219  -  cost:  0.09783457  - MSE:  28.982316430509968 - Train Accuracy:  0.9757576\n",
            "epoch:  220  -  cost:  0.13236412  - MSE:  26.12982930191722 - Train Accuracy:  0.95151514\n",
            "epoch:  221  -  cost:  0.16336155  - MSE:  30.13286273659014 - Train Accuracy:  0.92727274\n",
            "epoch:  222  -  cost:  0.2773628  - MSE:  22.658959453623122 - Train Accuracy:  0.8484849\n",
            "epoch:  223  -  cost:  0.2733269  - MSE:  23.881909487974674 - Train Accuracy:  0.8363636\n",
            "epoch:  224  -  cost:  0.2413684  - MSE:  15.963432211497723 - Train Accuracy:  0.8727273\n",
            "epoch:  225  -  cost:  0.1565065  - MSE:  15.405217896584434 - Train Accuracy:  0.95151514\n",
            "epoch:  226  -  cost:  0.10908133  - MSE:  18.68729553260387 - Train Accuracy:  0.9757576\n",
            "epoch:  227  -  cost:  0.093687974  - MSE:  22.54693056163983 - Train Accuracy:  0.9757576\n",
            "epoch:  228  -  cost:  0.09039295  - MSE:  23.742256130758044 - Train Accuracy:  0.9878788\n",
            "epoch:  229  -  cost:  0.0917454  - MSE:  26.137980729577457 - Train Accuracy:  0.969697\n",
            "epoch:  230  -  cost:  0.09487553  - MSE:  25.673583022196826 - Train Accuracy:  0.9818182\n",
            "epoch:  231  -  cost:  0.09906104  - MSE:  28.727964627026537 - Train Accuracy:  0.969697\n",
            "epoch:  232  -  cost:  0.12155663  - MSE:  25.775713918673958 - Train Accuracy:  0.96363634\n",
            "epoch:  233  -  cost:  0.13127851  - MSE:  29.32060974300595 - Train Accuracy:  0.95757574\n",
            "epoch:  234  -  cost:  0.1802088  - MSE:  23.93944435781406 - Train Accuracy:  0.8969697\n",
            "epoch:  235  -  cost:  0.18380536  - MSE:  26.602340253722623 - Train Accuracy:  0.93333334\n",
            "epoch:  236  -  cost:  0.18753822  - MSE:  20.256719624687225 - Train Accuracy:  0.9030303\n",
            "epoch:  237  -  cost:  0.14667787  - MSE:  20.727607404743804 - Train Accuracy:  0.96363634\n",
            "epoch:  238  -  cost:  0.09175742  - MSE:  21.926025199995923 - Train Accuracy:  0.9818182\n",
            "epoch:  239  -  cost:  0.08225628  - MSE:  25.129211648570216 - Train Accuracy:  0.9757576\n",
            "epoch:  240  -  cost:  0.075003095  - MSE:  26.328963939246336 - Train Accuracy:  0.9878788\n",
            "epoch:  241  -  cost:  0.07153762  - MSE:  28.463639359310637 - Train Accuracy:  0.9818182\n",
            "epoch:  242  -  cost:  0.06961702  - MSE:  28.76477986302165 - Train Accuracy:  0.9878788\n",
            "epoch:  243  -  cost:  0.06981084  - MSE:  31.065529566309245 - Train Accuracy:  0.9878788\n",
            "epoch:  244  -  cost:  0.07344889  - MSE:  30.05549198747476 - Train Accuracy:  0.9878788\n",
            "epoch:  245  -  cost:  0.077791035  - MSE:  32.89052832924425 - Train Accuracy:  0.9818182\n",
            "epoch:  246  -  cost:  0.103868276  - MSE:  29.911084547632992 - Train Accuracy:  0.96363634\n",
            "epoch:  247  -  cost:  0.12849934  - MSE:  33.77606633115115 - Train Accuracy:  0.95757574\n",
            "epoch:  248  -  cost:  0.27375957  - MSE:  27.53889832884055 - Train Accuracy:  0.8666667\n",
            "epoch:  249  -  cost:  0.4476224  - MSE:  33.44616325187574 - Train Accuracy:  0.75151515\n",
            "epoch:  250  -  cost:  0.58296084  - MSE:  15.434055479197283 - Train Accuracy:  0.7818182\n",
            "epoch:  251  -  cost:  0.3789148  - MSE:  6.362657008854308 - Train Accuracy:  0.830303\n",
            "epoch:  252  -  cost:  0.23030467  - MSE:  15.459016025652293 - Train Accuracy:  0.8787879\n",
            "epoch:  253  -  cost:  0.22851157  - MSE:  15.237166086535225 - Train Accuracy:  0.90909094\n",
            "epoch:  254  -  cost:  0.1781459  - MSE:  16.945714264274784 - Train Accuracy:  0.92727274\n",
            "epoch:  255  -  cost:  0.14911366  - MSE:  16.33576051451065 - Train Accuracy:  0.96363634\n",
            "epoch:  256  -  cost:  0.10885626  - MSE:  19.712494238205803 - Train Accuracy:  0.96363634\n",
            "epoch:  257  -  cost:  0.09540794  - MSE:  22.37255659852656 - Train Accuracy:  0.95757574\n",
            "epoch:  258  -  cost:  0.09060838  - MSE:  23.947650139359897 - Train Accuracy:  0.9878788\n",
            "epoch:  259  -  cost:  0.09046929  - MSE:  25.827457105921958 - Train Accuracy:  0.969697\n",
            "epoch:  260  -  cost:  0.10558072  - MSE:  25.72035417097368 - Train Accuracy:  0.9818182\n",
            "epoch:  261  -  cost:  0.11857806  - MSE:  27.82035834264223 - Train Accuracy:  0.95151514\n",
            "epoch:  262  -  cost:  0.17449473  - MSE:  25.929970562130148 - Train Accuracy:  0.90909094\n",
            "epoch:  263  -  cost:  0.2278653  - MSE:  27.38181737021078 - Train Accuracy:  0.8787879\n",
            "epoch:  264  -  cost:  0.24686143  - MSE:  20.56150410644676 - Train Accuracy:  0.8666667\n",
            "epoch:  265  -  cost:  0.20755471  - MSE:  18.714706063751347 - Train Accuracy:  0.92121214\n",
            "epoch:  266  -  cost:  0.08885665  - MSE:  21.329260590713925 - Train Accuracy:  0.9757576\n",
            "epoch:  267  -  cost:  0.0742378  - MSE:  24.854906430973003 - Train Accuracy:  0.9878788\n",
            "epoch:  268  -  cost:  0.06771426  - MSE:  27.441786906246367 - Train Accuracy:  0.9818182\n",
            "epoch:  269  -  cost:  0.063947186  - MSE:  29.078986787940288 - Train Accuracy:  0.9878788\n",
            "epoch:  270  -  cost:  0.060506515  - MSE:  30.777271864310023 - Train Accuracy:  0.9818182\n",
            "epoch:  271  -  cost:  0.05853567  - MSE:  31.57900967484796 - Train Accuracy:  0.9878788\n",
            "epoch:  272  -  cost:  0.057020865  - MSE:  32.79958621663752 - Train Accuracy:  0.9818182\n",
            "epoch:  273  -  cost:  0.055891696  - MSE:  32.464387400852715 - Train Accuracy:  0.9878788\n",
            "epoch:  274  -  cost:  0.05523847  - MSE:  33.6652914011181 - Train Accuracy:  0.9878788\n",
            "epoch:  275  -  cost:  0.055741224  - MSE:  33.265011234661266 - Train Accuracy:  0.9878788\n",
            "epoch:  276  -  cost:  0.05714016  - MSE:  34.97792903409159 - Train Accuracy:  0.9878788\n",
            "epoch:  277  -  cost:  0.058900155  - MSE:  33.95826932501384 - Train Accuracy:  0.9878788\n",
            "epoch:  278  -  cost:  0.0646174  - MSE:  36.49674462742766 - Train Accuracy:  0.9818182\n",
            "epoch:  279  -  cost:  0.07977248  - MSE:  33.98352625717173 - Train Accuracy:  0.9818182\n",
            "epoch:  280  -  cost:  0.117553696  - MSE:  39.19612570594929 - Train Accuracy:  0.95151514\n",
            "epoch:  281  -  cost:  0.3194097  - MSE:  32.63526318601694 - Train Accuracy:  0.8242424\n",
            "epoch:  282  -  cost:  0.7706439  - MSE:  41.28740374385001 - Train Accuracy:  0.6848485\n",
            "epoch:  283  -  cost:  1.1043501  - MSE:  18.312178600714713 - Train Accuracy:  0.6606061\n",
            "epoch:  284  -  cost:  0.74204946  - MSE:  2.3965954516101027 - Train Accuracy:  0.6\n",
            "epoch:  285  -  cost:  0.36896837  - MSE:  4.910007396172444 - Train Accuracy:  0.8484849\n",
            "epoch:  286  -  cost:  0.3194794  - MSE:  2.9743337493068642 - Train Accuracy:  0.8848485\n",
            "epoch:  287  -  cost:  0.29827407  - MSE:  5.926815593486928 - Train Accuracy:  0.8787879\n",
            "epoch:  288  -  cost:  0.2655989  - MSE:  3.768033968925043 - Train Accuracy:  0.92121214\n",
            "epoch:  289  -  cost:  0.25039956  - MSE:  8.185907667073346 - Train Accuracy:  0.9030303\n",
            "epoch:  290  -  cost:  0.2667804  - MSE:  4.3604190006046295 - Train Accuracy:  0.92121214\n",
            "epoch:  291  -  cost:  0.27804965  - MSE:  12.8475751954505 - Train Accuracy:  0.8848485\n",
            "epoch:  292  -  cost:  0.42491484  - MSE:  5.058849245960643 - Train Accuracy:  0.7818182\n",
            "epoch:  293  -  cost:  0.3572277  - MSE:  18.054066122935886 - Train Accuracy:  0.8424242\n",
            "epoch:  294  -  cost:  0.47004828  - MSE:  5.175332131509729 - Train Accuracy:  0.75151515\n",
            "epoch:  295  -  cost:  0.21577851  - MSE:  12.837412285531492 - Train Accuracy:  0.92121214\n",
            "epoch:  296  -  cost:  0.1803985  - MSE:  7.959467320393512 - Train Accuracy:  0.96363634\n",
            "epoch:  297  -  cost:  0.14130999  - MSE:  13.011007536584941 - Train Accuracy:  0.969697\n",
            "epoch:  298  -  cost:  0.124594904  - MSE:  12.909924454539537 - Train Accuracy:  0.9757576\n",
            "epoch:  299  -  cost:  0.109541714  - MSE:  16.333298427621838 - Train Accuracy:  0.9757576\n",
            "epoch:  300  -  cost:  0.09889253  - MSE:  16.422053834221693 - Train Accuracy:  0.9757576\n",
            "epoch:  301  -  cost:  0.08924456  - MSE:  19.84018882140974 - Train Accuracy:  0.9818182\n",
            "epoch:  302  -  cost:  0.08574186  - MSE:  20.02536858964094 - Train Accuracy:  0.9757576\n",
            "epoch:  303  -  cost:  0.083108105  - MSE:  23.18049737709943 - Train Accuracy:  0.9878788\n",
            "epoch:  304  -  cost:  0.08701574  - MSE:  22.314957311662457 - Train Accuracy:  0.9757576\n",
            "epoch:  305  -  cost:  0.100117154  - MSE:  25.535412604420884 - Train Accuracy:  0.9757576\n",
            "epoch:  306  -  cost:  0.12998255  - MSE:  22.8969639134374 - Train Accuracy:  0.95151514\n",
            "epoch:  307  -  cost:  0.17698357  - MSE:  27.936793023704542 - Train Accuracy:  0.9030303\n",
            "epoch:  308  -  cost:  0.3403009  - MSE:  24.303709068885844 - Train Accuracy:  0.8242424\n",
            "epoch:  309  -  cost:  0.32128894  - MSE:  23.313280932064167 - Train Accuracy:  0.830303\n",
            "epoch:  310  -  cost:  0.38444647  - MSE:  14.015033490897698 - Train Accuracy:  0.75151515\n",
            "epoch:  311  -  cost:  0.14032403  - MSE:  14.391821169777407 - Train Accuracy:  0.95151514\n",
            "epoch:  312  -  cost:  0.1174931  - MSE:  15.491362250041897 - Train Accuracy:  0.9757576\n",
            "epoch:  313  -  cost:  0.09477113  - MSE:  19.77343104543906 - Train Accuracy:  0.9818182\n",
            "epoch:  314  -  cost:  0.081834085  - MSE:  21.41996769716422 - Train Accuracy:  0.9878788\n",
            "epoch:  315  -  cost:  0.07309379  - MSE:  24.155686928339236 - Train Accuracy:  0.9878788\n",
            "epoch:  316  -  cost:  0.06761374  - MSE:  25.92788395414334 - Train Accuracy:  0.9939394\n",
            "epoch:  317  -  cost:  0.06330687  - MSE:  27.686596050864896 - Train Accuracy:  0.9939394\n",
            "epoch:  318  -  cost:  0.060769856  - MSE:  29.023563157353486 - Train Accuracy:  0.9939394\n",
            "epoch:  319  -  cost:  0.05890275  - MSE:  29.755764727200646 - Train Accuracy:  0.9939394\n",
            "epoch:  320  -  cost:  0.057726208  - MSE:  30.2224349318129 - Train Accuracy:  0.9939394\n",
            "epoch:  321  -  cost:  0.057034504  - MSE:  30.853203530838147 - Train Accuracy:  0.9939394\n",
            "epoch:  322  -  cost:  0.057180885  - MSE:  31.11199191478605 - Train Accuracy:  0.9939394\n",
            "epoch:  323  -  cost:  0.05932793  - MSE:  31.48228079344746 - Train Accuracy:  0.9878788\n",
            "epoch:  324  -  cost:  0.062132508  - MSE:  32.134299335992125 - Train Accuracy:  0.9757576\n",
            "epoch:  325  -  cost:  0.068654604  - MSE:  31.57970955619322 - Train Accuracy:  0.9818182\n",
            "epoch:  326  -  cost:  0.07391959  - MSE:  32.24725723032233 - Train Accuracy:  0.9757576\n",
            "epoch:  327  -  cost:  0.08788117  - MSE:  31.704226995182456 - Train Accuracy:  0.9818182\n",
            "epoch:  328  -  cost:  0.09190633  - MSE:  30.796961569750724 - Train Accuracy:  0.969697\n",
            "epoch:  329  -  cost:  0.10672836  - MSE:  31.232545903032854 - Train Accuracy:  0.969697\n",
            "epoch:  330  -  cost:  0.12521045  - MSE:  30.225691072284718 - Train Accuracy:  0.96363634\n",
            "epoch:  331  -  cost:  0.11692492  - MSE:  31.388495531046786 - Train Accuracy:  0.969697\n",
            "epoch:  332  -  cost:  0.12776423  - MSE:  29.56873464725399 - Train Accuracy:  0.95757574\n",
            "epoch:  333  -  cost:  0.12321109  - MSE:  31.027718415696388 - Train Accuracy:  0.95757574\n",
            "epoch:  334  -  cost:  0.1501013  - MSE:  29.27419228903316 - Train Accuracy:  0.95757574\n",
            "epoch:  335  -  cost:  0.10382908  - MSE:  30.300158615315436 - Train Accuracy:  0.969697\n",
            "epoch:  336  -  cost:  0.08817437  - MSE:  28.302142095534244 - Train Accuracy:  0.9757576\n",
            "epoch:  337  -  cost:  0.06985154  - MSE:  29.503943864450342 - Train Accuracy:  0.9878788\n",
            "epoch:  338  -  cost:  0.06138708  - MSE:  29.371396525048016 - Train Accuracy:  0.9818182\n",
            "epoch:  339  -  cost:  0.049497783  - MSE:  31.349102654225558 - Train Accuracy:  0.9878788\n",
            "epoch:  340  -  cost:  0.047327165  - MSE:  32.53531426077963 - Train Accuracy:  0.9939394\n",
            "epoch:  341  -  cost:  0.044902977  - MSE:  33.00516889020149 - Train Accuracy:  0.9939394\n",
            "epoch:  342  -  cost:  0.044127006  - MSE:  33.66078471877169 - Train Accuracy:  0.9939394\n",
            "epoch:  343  -  cost:  0.042903095  - MSE:  34.03861772845316 - Train Accuracy:  0.9939394\n",
            "epoch:  344  -  cost:  0.042439554  - MSE:  34.589122697984315 - Train Accuracy:  0.9939394\n",
            "epoch:  345  -  cost:  0.041641627  - MSE:  34.93977899762599 - Train Accuracy:  0.9939394\n",
            "epoch:  346  -  cost:  0.041403145  - MSE:  35.259658107939835 - Train Accuracy:  0.9939394\n",
            "epoch:  347  -  cost:  0.040903073  - MSE:  35.603231577743124 - Train Accuracy:  0.9939394\n",
            "epoch:  348  -  cost:  0.041281298  - MSE:  35.84511177785752 - Train Accuracy:  1.0\n",
            "epoch:  349  -  cost:  0.04079579  - MSE:  36.1134044021433 - Train Accuracy:  0.9939394\n",
            "epoch:  350  -  cost:  0.041738074  - MSE:  36.436822115825535 - Train Accuracy:  1.0\n",
            "epoch:  351  -  cost:  0.04149884  - MSE:  36.57138815228708 - Train Accuracy:  0.9939394\n",
            "epoch:  352  -  cost:  0.04244219  - MSE:  36.87848542142579 - Train Accuracy:  1.0\n",
            "epoch:  353  -  cost:  0.041367054  - MSE:  36.98803870863488 - Train Accuracy:  0.9939394\n",
            "epoch:  354  -  cost:  0.04200249  - MSE:  37.15665114185471 - Train Accuracy:  1.0\n",
            "epoch:  355  -  cost:  0.040661633  - MSE:  37.274704591801076 - Train Accuracy:  0.9939394\n",
            "epoch:  356  -  cost:  0.04123889  - MSE:  37.49031617468797 - Train Accuracy:  1.0\n",
            "epoch:  357  -  cost:  0.040492505  - MSE:  37.670294710462244 - Train Accuracy:  0.9939394\n",
            "epoch:  358  -  cost:  0.041062582  - MSE:  37.93823471984634 - Train Accuracy:  1.0\n",
            "epoch:  359  -  cost:  0.039904475  - MSE:  38.0848294943223 - Train Accuracy:  0.9939394\n",
            "epoch:  360  -  cost:  0.039721757  - MSE:  38.47311237119802 - Train Accuracy:  1.0\n",
            "epoch:  361  -  cost:  0.040009905  - MSE:  38.41295930124319 - Train Accuracy:  0.9939394\n",
            "epoch:  362  -  cost:  0.03995798  - MSE:  38.74740681244764 - Train Accuracy:  1.0\n",
            "epoch:  363  -  cost:  0.038595784  - MSE:  38.93961375495611 - Train Accuracy:  0.9939394\n",
            "epoch:  364  -  cost:  0.03809703  - MSE:  39.29032089623146 - Train Accuracy:  1.0\n",
            "epoch:  365  -  cost:  0.037117496  - MSE:  39.43281117049972 - Train Accuracy:  0.9939394\n",
            "epoch:  366  -  cost:  0.036445763  - MSE:  39.84051892955505 - Train Accuracy:  1.0\n",
            "epoch:  367  -  cost:  0.035230506  - MSE:  40.08411589015512 - Train Accuracy:  0.9939394\n",
            "epoch:  368  -  cost:  0.034867357  - MSE:  40.61488147098161 - Train Accuracy:  1.0\n",
            "epoch:  369  -  cost:  0.033937626  - MSE:  40.7824591217495 - Train Accuracy:  0.9939394\n",
            "epoch:  370  -  cost:  0.033112653  - MSE:  41.25634673561756 - Train Accuracy:  1.0\n",
            "epoch:  371  -  cost:  0.03151974  - MSE:  41.308045638705664 - Train Accuracy:  1.0\n",
            "epoch:  372  -  cost:  0.030961426  - MSE:  41.66893296981518 - Train Accuracy:  1.0\n",
            "epoch:  373  -  cost:  0.03007458  - MSE:  41.93223746399152 - Train Accuracy:  1.0\n",
            "epoch:  374  -  cost:  0.029556809  - MSE:  42.35086371529472 - Train Accuracy:  1.0\n",
            "epoch:  375  -  cost:  0.02872545  - MSE:  42.53108468141927 - Train Accuracy:  1.0\n",
            "epoch:  376  -  cost:  0.028511455  - MSE:  42.88107371665884 - Train Accuracy:  1.0\n",
            "epoch:  377  -  cost:  0.027737742  - MSE:  43.01327835804701 - Train Accuracy:  1.0\n",
            "epoch:  378  -  cost:  0.027175441  - MSE:  43.36807989430609 - Train Accuracy:  1.0\n",
            "epoch:  379  -  cost:  0.026735635  - MSE:  43.60241728181703 - Train Accuracy:  1.0\n",
            "epoch:  380  -  cost:  0.026388403  - MSE:  44.039115172554865 - Train Accuracy:  1.0\n",
            "epoch:  381  -  cost:  0.025984496  - MSE:  44.310848822966385 - Train Accuracy:  1.0\n",
            "epoch:  382  -  cost:  0.02561674  - MSE:  44.6843024315311 - Train Accuracy:  1.0\n",
            "epoch:  383  -  cost:  0.025339881  - MSE:  45.12215268389871 - Train Accuracy:  1.0\n",
            "epoch:  384  -  cost:  0.02506643  - MSE:  45.335491906900906 - Train Accuracy:  1.0\n",
            "epoch:  385  -  cost:  0.024775475  - MSE:  45.65733177473506 - Train Accuracy:  1.0\n",
            "epoch:  386  -  cost:  0.024530988  - MSE:  46.028050442179385 - Train Accuracy:  1.0\n",
            "epoch:  387  -  cost:  0.024167322  - MSE:  46.16546959054703 - Train Accuracy:  1.0\n",
            "epoch:  388  -  cost:  0.02389064  - MSE:  46.6505187042356 - Train Accuracy:  1.0\n",
            "epoch:  389  -  cost:  0.023558065  - MSE:  46.65010390570198 - Train Accuracy:  1.0\n",
            "epoch:  390  -  cost:  0.023328206  - MSE:  46.902226293762155 - Train Accuracy:  1.0\n",
            "epoch:  391  -  cost:  0.023142839  - MSE:  47.10116214226187 - Train Accuracy:  1.0\n",
            "epoch:  392  -  cost:  0.023074832  - MSE:  47.35453461845587 - Train Accuracy:  1.0\n",
            "epoch:  393  -  cost:  0.022849577  - MSE:  47.567372726152406 - Train Accuracy:  1.0\n",
            "epoch:  394  -  cost:  0.02281192  - MSE:  47.768984413331864 - Train Accuracy:  1.0\n",
            "epoch:  395  -  cost:  0.022750624  - MSE:  48.04354949564088 - Train Accuracy:  1.0\n",
            "epoch:  396  -  cost:  0.023356682  - MSE:  48.28736355287698 - Train Accuracy:  1.0\n",
            "epoch:  397  -  cost:  0.022936298  - MSE:  48.40820917537387 - Train Accuracy:  1.0\n",
            "epoch:  398  -  cost:  0.023703117  - MSE:  48.74265254960797 - Train Accuracy:  1.0\n",
            "epoch:  399  -  cost:  0.022887588  - MSE:  48.89355865582082 - Train Accuracy:  1.0\n",
            "epoch:  400  -  cost:  0.023009647  - MSE:  49.227900869248266 - Train Accuracy:  1.0\n",
            "epoch:  401  -  cost:  0.022502758  - MSE:  49.12512099078111 - Train Accuracy:  1.0\n",
            "epoch:  402  -  cost:  0.023119135  - MSE:  49.44155672417243 - Train Accuracy:  1.0\n",
            "epoch:  403  -  cost:  0.022186166  - MSE:  49.52362024009402 - Train Accuracy:  1.0\n",
            "epoch:  404  -  cost:  0.022101792  - MSE:  49.82764686624124 - Train Accuracy:  1.0\n",
            "epoch:  405  -  cost:  0.021390699  - MSE:  49.7806577811096 - Train Accuracy:  1.0\n",
            "epoch:  406  -  cost:  0.021962138  - MSE:  50.148815554504935 - Train Accuracy:  1.0\n",
            "epoch:  407  -  cost:  0.020928223  - MSE:  50.20713036146276 - Train Accuracy:  1.0\n",
            "epoch:  408  -  cost:  0.020438269  - MSE:  50.56011950096197 - Train Accuracy:  1.0\n",
            "epoch:  409  -  cost:  0.019946996  - MSE:  50.6092751582263 - Train Accuracy:  1.0\n",
            "epoch:  410  -  cost:  0.019452147  - MSE:  50.9887252228486 - Train Accuracy:  1.0\n",
            "epoch:  411  -  cost:  0.019175386  - MSE:  51.16394377443395 - Train Accuracy:  1.0\n",
            "epoch:  412  -  cost:  0.019111339  - MSE:  51.470154654348576 - Train Accuracy:  1.0\n",
            "epoch:  413  -  cost:  0.018768707  - MSE:  51.44950936489183 - Train Accuracy:  1.0\n",
            "epoch:  414  -  cost:  0.018721908  - MSE:  51.79936233460549 - Train Accuracy:  1.0\n",
            "epoch:  415  -  cost:  0.018345024  - MSE:  51.7669445297693 - Train Accuracy:  1.0\n",
            "epoch:  416  -  cost:  0.018479608  - MSE:  52.04468624502163 - Train Accuracy:  1.0\n",
            "epoch:  417  -  cost:  0.018103706  - MSE:  51.92751807578322 - Train Accuracy:  1.0\n",
            "epoch:  418  -  cost:  0.017802628  - MSE:  52.368864313305345 - Train Accuracy:  1.0\n",
            "epoch:  419  -  cost:  0.017613633  - MSE:  52.252564922297616 - Train Accuracy:  1.0\n",
            "epoch:  420  -  cost:  0.017456464  - MSE:  52.63219461700807 - Train Accuracy:  1.0\n",
            "epoch:  421  -  cost:  0.017373431  - MSE:  52.71960662140249 - Train Accuracy:  1.0\n",
            "epoch:  422  -  cost:  0.017263338  - MSE:  53.002518731442194 - Train Accuracy:  1.0\n",
            "epoch:  423  -  cost:  0.017194508  - MSE:  53.0391296412817 - Train Accuracy:  1.0\n",
            "epoch:  424  -  cost:  0.017413588  - MSE:  53.21343964559769 - Train Accuracy:  1.0\n",
            "epoch:  425  -  cost:  0.017235057  - MSE:  53.108164677961604 - Train Accuracy:  1.0\n",
            "epoch:  426  -  cost:  0.01740423  - MSE:  53.414871106807695 - Train Accuracy:  1.0\n",
            "epoch:  427  -  cost:  0.017073303  - MSE:  53.29075884478061 - Train Accuracy:  1.0\n",
            "epoch:  428  -  cost:  0.017275339  - MSE:  53.6036520577807 - Train Accuracy:  1.0\n",
            "epoch:  429  -  cost:  0.016732283  - MSE:  53.48410080834953 - Train Accuracy:  1.0\n",
            "epoch:  430  -  cost:  0.01696981  - MSE:  53.81166795796253 - Train Accuracy:  1.0\n",
            "epoch:  431  -  cost:  0.016523734  - MSE:  53.71675263749739 - Train Accuracy:  1.0\n",
            "epoch:  432  -  cost:  0.016691515  - MSE:  53.91439490372828 - Train Accuracy:  1.0\n",
            "epoch:  433  -  cost:  0.016231705  - MSE:  53.94644712920955 - Train Accuracy:  1.0\n",
            "epoch:  434  -  cost:  0.016084203  - MSE:  54.30804765728631 - Train Accuracy:  1.0\n",
            "epoch:  435  -  cost:  0.015885836  - MSE:  54.30582540531687 - Train Accuracy:  1.0\n",
            "epoch:  436  -  cost:  0.015725298  - MSE:  54.46708511759219 - Train Accuracy:  1.0\n",
            "epoch:  437  -  cost:  0.01555624  - MSE:  54.49173017643055 - Train Accuracy:  1.0\n",
            "epoch:  438  -  cost:  0.015424266  - MSE:  54.86509982102742 - Train Accuracy:  1.0\n",
            "epoch:  439  -  cost:  0.0153374905  - MSE:  54.898587035586765 - Train Accuracy:  1.0\n",
            "epoch:  440  -  cost:  0.015560303  - MSE:  55.05066299128233 - Train Accuracy:  1.0\n",
            "epoch:  441  -  cost:  0.015345293  - MSE:  54.89296876816031 - Train Accuracy:  1.0\n",
            "epoch:  442  -  cost:  0.015446901  - MSE:  55.22584217154351 - Train Accuracy:  1.0\n",
            "epoch:  443  -  cost:  0.015154457  - MSE:  55.26090907253361 - Train Accuracy:  1.0\n",
            "epoch:  444  -  cost:  0.01534518  - MSE:  55.5287922022701 - Train Accuracy:  1.0\n",
            "epoch:  445  -  cost:  0.014998417  - MSE:  55.35176626541803 - Train Accuracy:  1.0\n",
            "epoch:  446  -  cost:  0.015157159  - MSE:  55.729973747807676 - Train Accuracy:  1.0\n",
            "epoch:  447  -  cost:  0.014730665  - MSE:  55.63927527937097 - Train Accuracy:  1.0\n",
            "epoch:  448  -  cost:  0.014902899  - MSE:  55.96734938235453 - Train Accuracy:  1.0\n",
            "epoch:  449  -  cost:  0.014533998  - MSE:  55.78642287963402 - Train Accuracy:  1.0\n",
            "epoch:  450  -  cost:  0.014509955  - MSE:  56.08567316682801 - Train Accuracy:  1.0\n",
            "epoch:  451  -  cost:  0.014326886  - MSE:  56.21802117395243 - Train Accuracy:  1.0\n",
            "epoch:  452  -  cost:  0.014305973  - MSE:  56.33677827028458 - Train Accuracy:  1.0\n",
            "epoch:  453  -  cost:  0.014139193  - MSE:  56.39451481564507 - Train Accuracy:  1.0\n",
            "epoch:  454  -  cost:  0.01430784  - MSE:  56.593380622189656 - Train Accuracy:  1.0\n",
            "epoch:  455  -  cost:  0.014068469  - MSE:  56.58904356603437 - Train Accuracy:  1.0\n",
            "epoch:  456  -  cost:  0.014033823  - MSE:  56.79848222435075 - Train Accuracy:  1.0\n",
            "epoch:  457  -  cost:  0.013918164  - MSE:  56.814239874098824 - Train Accuracy:  1.0\n",
            "epoch:  458  -  cost:  0.014010524  - MSE:  56.9314940311922 - Train Accuracy:  1.0\n",
            "epoch:  459  -  cost:  0.013700331  - MSE:  56.87372958141625 - Train Accuracy:  1.0\n",
            "epoch:  460  -  cost:  0.013795048  - MSE:  57.11359671070731 - Train Accuracy:  1.0\n",
            "epoch:  461  -  cost:  0.013513093  - MSE:  57.17025797233203 - Train Accuracy:  1.0\n",
            "epoch:  462  -  cost:  0.013472135  - MSE:  57.44706034480477 - Train Accuracy:  1.0\n",
            "epoch:  463  -  cost:  0.01335142  - MSE:  57.4594470823585 - Train Accuracy:  1.0\n",
            "epoch:  464  -  cost:  0.013321894  - MSE:  57.65514811524885 - Train Accuracy:  1.0\n",
            "epoch:  465  -  cost:  0.013151949  - MSE:  57.723622080525544 - Train Accuracy:  1.0\n",
            "epoch:  466  -  cost:  0.013197506  - MSE:  57.76470911236393 - Train Accuracy:  1.0\n",
            "epoch:  467  -  cost:  0.012983424  - MSE:  57.85870215643889 - Train Accuracy:  1.0\n",
            "epoch:  468  -  cost:  0.012943192  - MSE:  58.080685345781795 - Train Accuracy:  1.0\n",
            "epoch:  469  -  cost:  0.012834936  - MSE:  58.18226479471503 - Train Accuracy:  1.0\n",
            "epoch:  470  -  cost:  0.012705202  - MSE:  58.23447658231913 - Train Accuracy:  1.0\n",
            "epoch:  471  -  cost:  0.012632313  - MSE:  58.42252620094211 - Train Accuracy:  1.0\n",
            "epoch:  472  -  cost:  0.012566925  - MSE:  58.535013391067274 - Train Accuracy:  1.0\n",
            "epoch:  473  -  cost:  0.012485838  - MSE:  58.605688108598336 - Train Accuracy:  1.0\n",
            "epoch:  474  -  cost:  0.012435494  - MSE:  58.62202601209726 - Train Accuracy:  1.0\n",
            "epoch:  475  -  cost:  0.012395658  - MSE:  58.77196406361539 - Train Accuracy:  1.0\n",
            "epoch:  476  -  cost:  0.012334711  - MSE:  58.88824091231213 - Train Accuracy:  1.0\n",
            "epoch:  477  -  cost:  0.012344615  - MSE:  58.93476964288605 - Train Accuracy:  1.0\n",
            "epoch:  478  -  cost:  0.01222223  - MSE:  58.92546621036712 - Train Accuracy:  1.0\n",
            "epoch:  479  -  cost:  0.012266699  - MSE:  59.265973154145925 - Train Accuracy:  1.0\n",
            "epoch:  480  -  cost:  0.012085493  - MSE:  59.26320156781986 - Train Accuracy:  1.0\n",
            "epoch:  481  -  cost:  0.012138262  - MSE:  59.46083601529866 - Train Accuracy:  1.0\n",
            "epoch:  482  -  cost:  0.011972795  - MSE:  59.39745097832084 - Train Accuracy:  1.0\n",
            "epoch:  483  -  cost:  0.012057338  - MSE:  59.6475281277516 - Train Accuracy:  1.0\n",
            "epoch:  484  -  cost:  0.01184753  - MSE:  59.656598869302464 - Train Accuracy:  1.0\n",
            "epoch:  485  -  cost:  0.011887703  - MSE:  59.716408756734275 - Train Accuracy:  1.0\n",
            "epoch:  486  -  cost:  0.011769712  - MSE:  59.68589519443077 - Train Accuracy:  1.0\n",
            "epoch:  487  -  cost:  0.0116918655  - MSE:  59.973484894196105 - Train Accuracy:  1.0\n",
            "epoch:  488  -  cost:  0.011642321  - MSE:  59.95887768743408 - Train Accuracy:  1.0\n",
            "epoch:  489  -  cost:  0.011696816  - MSE:  60.20773097279008 - Train Accuracy:  1.0\n",
            "epoch:  490  -  cost:  0.011491881  - MSE:  60.227070733497776 - Train Accuracy:  1.0\n",
            "epoch:  491  -  cost:  0.011464536  - MSE:  60.528597131535705 - Train Accuracy:  1.0\n",
            "epoch:  492  -  cost:  0.011397749  - MSE:  60.47737208319883 - Train Accuracy:  1.0\n",
            "epoch:  493  -  cost:  0.0114554465  - MSE:  60.77610063500814 - Train Accuracy:  1.0\n",
            "epoch:  494  -  cost:  0.011267872  - MSE:  60.67379855888325 - Train Accuracy:  1.0\n",
            "epoch:  495  -  cost:  0.011207376  - MSE:  60.80526519178013 - Train Accuracy:  1.0\n",
            "epoch:  496  -  cost:  0.011146672  - MSE:  60.99224971329663 - Train Accuracy:  1.0\n",
            "epoch:  497  -  cost:  0.011109468  - MSE:  61.071751514795245 - Train Accuracy:  1.0\n",
            "epoch:  498  -  cost:  0.011015805  - MSE:  61.177692498466534 - Train Accuracy:  1.0\n",
            "epoch:  499  -  cost:  0.0109685995  - MSE:  61.36643154262356 - Train Accuracy:  1.0\n",
            "epoch:  500  -  cost:  0.010881713  - MSE:  61.45547594454435 - Train Accuracy:  1.0\n",
            "epoch:  501  -  cost:  0.010830413  - MSE:  61.54491355216679 - Train Accuracy:  1.0\n",
            "epoch:  502  -  cost:  0.010772584  - MSE:  61.694139926271916 - Train Accuracy:  1.0\n",
            "epoch:  503  -  cost:  0.010827797  - MSE:  61.77596093544354 - Train Accuracy:  1.0\n",
            "epoch:  504  -  cost:  0.010739174  - MSE:  61.75589124052121 - Train Accuracy:  1.0\n",
            "epoch:  505  -  cost:  0.010727852  - MSE:  62.02915098561824 - Train Accuracy:  1.0\n",
            "epoch:  506  -  cost:  0.010626433  - MSE:  61.93616707914349 - Train Accuracy:  1.0\n",
            "epoch:  507  -  cost:  0.010632497  - MSE:  62.193438780661836 - Train Accuracy:  1.0\n",
            "epoch:  508  -  cost:  0.010506712  - MSE:  62.17219407572568 - Train Accuracy:  1.0\n",
            "epoch:  509  -  cost:  0.0105287  - MSE:  62.46663614883689 - Train Accuracy:  1.0\n",
            "epoch:  510  -  cost:  0.010396649  - MSE:  62.472694822887725 - Train Accuracy:  1.0\n",
            "epoch:  511  -  cost:  0.010399006  - MSE:  62.571872666518196 - Train Accuracy:  1.0\n",
            "epoch:  512  -  cost:  0.010295804  - MSE:  62.59827924355463 - Train Accuracy:  1.0\n",
            "epoch:  513  -  cost:  0.010250008  - MSE:  62.82364038760575 - Train Accuracy:  1.0\n",
            "epoch:  514  -  cost:  0.010197902  - MSE:  62.82558552559031 - Train Accuracy:  1.0\n",
            "epoch:  515  -  cost:  0.010203467  - MSE:  63.02147122690805 - Train Accuracy:  1.0\n",
            "epoch:  516  -  cost:  0.010097487  - MSE:  63.00843669190823 - Train Accuracy:  1.0\n",
            "epoch:  517  -  cost:  0.010042875  - MSE:  63.234925834499236 - Train Accuracy:  1.0\n",
            "epoch:  518  -  cost:  0.010018076  - MSE:  63.30299231882108 - Train Accuracy:  1.0\n",
            "epoch:  519  -  cost:  0.010009598  - MSE:  63.468337985598076 - Train Accuracy:  1.0\n",
            "epoch:  520  -  cost:  0.009923469  - MSE:  63.3621895059203 - Train Accuracy:  1.0\n",
            "epoch:  521  -  cost:  0.00985275  - MSE:  63.63109770235339 - Train Accuracy:  1.0\n",
            "epoch:  522  -  cost:  0.009831776  - MSE:  63.69712062088621 - Train Accuracy:  1.0\n",
            "epoch:  523  -  cost:  0.009796938  - MSE:  63.82390892078911 - Train Accuracy:  1.0\n",
            "epoch:  524  -  cost:  0.009737474  - MSE:  63.922179700274995 - Train Accuracy:  1.0\n",
            "epoch:  525  -  cost:  0.009786236  - MSE:  64.04115384573136 - Train Accuracy:  1.0\n",
            "epoch:  526  -  cost:  0.009648041  - MSE:  64.12916797260934 - Train Accuracy:  1.0\n",
            "epoch:  527  -  cost:  0.009685492  - MSE:  64.24647629470537 - Train Accuracy:  1.0\n",
            "epoch:  528  -  cost:  0.009551802  - MSE:  64.31347699451673 - Train Accuracy:  1.0\n",
            "epoch:  529  -  cost:  0.009539967  - MSE:  64.45890693833628 - Train Accuracy:  1.0\n",
            "epoch:  530  -  cost:  0.009470205  - MSE:  64.53117834215192 - Train Accuracy:  1.0\n",
            "epoch:  531  -  cost:  0.009426808  - MSE:  64.51516875540881 - Train Accuracy:  1.0\n",
            "epoch:  532  -  cost:  0.009377925  - MSE:  64.6113339874168 - Train Accuracy:  1.0\n",
            "epoch:  533  -  cost:  0.009349007  - MSE:  64.78039728254439 - Train Accuracy:  1.0\n",
            "epoch:  534  -  cost:  0.009297597  - MSE:  64.88127268505544 - Train Accuracy:  1.0\n",
            "epoch:  535  -  cost:  0.009302228  - MSE:  64.97715977232318 - Train Accuracy:  1.0\n",
            "epoch:  536  -  cost:  0.009237665  - MSE:  64.95593275691856 - Train Accuracy:  1.0\n",
            "epoch:  537  -  cost:  0.009229262  - MSE:  65.19182773248797 - Train Accuracy:  1.0\n",
            "epoch:  538  -  cost:  0.009138361  - MSE:  65.10954788708312 - Train Accuracy:  1.0\n",
            "epoch:  539  -  cost:  0.009160682  - MSE:  65.32257845187111 - Train Accuracy:  1.0\n",
            "epoch:  540  -  cost:  0.009054392  - MSE:  65.31120608429175 - Train Accuracy:  1.0\n",
            "epoch:  541  -  cost:  0.009004023  - MSE:  65.47261879799551 - Train Accuracy:  1.0\n",
            "epoch:  542  -  cost:  0.008999688  - MSE:  65.41777032725841 - Train Accuracy:  1.0\n",
            "epoch:  543  -  cost:  0.008985463  - MSE:  65.65378897448784 - Train Accuracy:  1.0\n",
            "epoch:  544  -  cost:  0.008912407  - MSE:  65.63019407052158 - Train Accuracy:  1.0\n",
            "epoch:  545  -  cost:  0.008856594  - MSE:  65.8131457561811 - Train Accuracy:  1.0\n",
            "epoch:  546  -  cost:  0.008812619  - MSE:  65.8908732011057 - Train Accuracy:  1.0\n",
            "epoch:  547  -  cost:  0.008800024  - MSE:  66.03719502402808 - Train Accuracy:  1.0\n",
            "epoch:  548  -  cost:  0.008782037  - MSE:  66.14897728689273 - Train Accuracy:  1.0\n",
            "epoch:  549  -  cost:  0.008799365  - MSE:  66.3172049340615 - Train Accuracy:  1.0\n",
            "epoch:  550  -  cost:  0.008675273  - MSE:  66.28626962655015 - Train Accuracy:  1.0\n",
            "epoch:  551  -  cost:  0.008678809  - MSE:  66.33936613415229 - Train Accuracy:  1.0\n",
            "epoch:  552  -  cost:  0.008608346  - MSE:  66.33647310315644 - Train Accuracy:  1.0\n",
            "epoch:  553  -  cost:  0.008555832  - MSE:  66.51798897279919 - Train Accuracy:  1.0\n",
            "epoch:  554  -  cost:  0.008513791  - MSE:  66.62635672204794 - Train Accuracy:  1.0\n",
            "epoch:  555  -  cost:  0.008495486  - MSE:  66.73235341974711 - Train Accuracy:  1.0\n",
            "epoch:  556  -  cost:  0.008463207  - MSE:  66.80345613277463 - Train Accuracy:  1.0\n",
            "epoch:  557  -  cost:  0.008500671  - MSE:  66.98948935894636 - Train Accuracy:  1.0\n",
            "epoch:  558  -  cost:  0.008382495  - MSE:  66.96050680153891 - Train Accuracy:  1.0\n",
            "epoch:  559  -  cost:  0.008362404  - MSE:  67.17158193850534 - Train Accuracy:  1.0\n",
            "epoch:  560  -  cost:  0.008349908  - MSE:  67.10610438326091 - Train Accuracy:  1.0\n",
            "epoch:  561  -  cost:  0.008340076  - MSE:  67.30428189307155 - Train Accuracy:  1.0\n",
            "epoch:  562  -  cost:  0.008259772  - MSE:  67.23352752571557 - Train Accuracy:  1.0\n",
            "epoch:  563  -  cost:  0.008218382  - MSE:  67.45558055110095 - Train Accuracy:  1.0\n",
            "epoch:  564  -  cost:  0.008185517  - MSE:  67.46065873077171 - Train Accuracy:  1.0\n",
            "epoch:  565  -  cost:  0.008188179  - MSE:  67.55011163423244 - Train Accuracy:  1.0\n",
            "epoch:  566  -  cost:  0.00812518  - MSE:  67.5556086455832 - Train Accuracy:  1.0\n",
            "epoch:  567  -  cost:  0.008081853  - MSE:  67.76239216833476 - Train Accuracy:  1.0\n",
            "epoch:  568  -  cost:  0.008086741  - MSE:  67.76685843401621 - Train Accuracy:  1.0\n",
            "epoch:  569  -  cost:  0.008057084  - MSE:  67.92706808871856 - Train Accuracy:  1.0\n",
            "epoch:  570  -  cost:  0.007999791  - MSE:  67.93166437099218 - Train Accuracy:  1.0\n",
            "epoch:  571  -  cost:  0.007998862  - MSE:  68.10961617710515 - Train Accuracy:  1.0\n",
            "epoch:  572  -  cost:  0.007942435  - MSE:  68.04342504736431 - Train Accuracy:  1.0\n",
            "epoch:  573  -  cost:  0.007894273  - MSE:  68.27381900164413 - Train Accuracy:  1.0\n",
            "epoch:  574  -  cost:  0.007856514  - MSE:  68.34818119689446 - Train Accuracy:  1.0\n",
            "epoch:  575  -  cost:  0.007844931  - MSE:  68.50309023951671 - Train Accuracy:  1.0\n",
            "epoch:  576  -  cost:  0.007826566  - MSE:  68.43763772524795 - Train Accuracy:  1.0\n",
            "epoch:  577  -  cost:  0.007821949  - MSE:  68.63107336938948 - Train Accuracy:  1.0\n",
            "epoch:  578  -  cost:  0.0077503854  - MSE:  68.6472011410572 - Train Accuracy:  1.0\n",
            "epoch:  579  -  cost:  0.007759519  - MSE:  68.78009594623562 - Train Accuracy:  1.0\n",
            "epoch:  580  -  cost:  0.0076891696  - MSE:  68.7714466362698 - Train Accuracy:  1.0\n",
            "epoch:  581  -  cost:  0.0076464363  - MSE:  68.9454695825476 - Train Accuracy:  1.0\n",
            "epoch:  582  -  cost:  0.007643617  - MSE:  68.951302527694 - Train Accuracy:  1.0\n",
            "epoch:  583  -  cost:  0.0076437327  - MSE:  69.13139675941032 - Train Accuracy:  1.0\n",
            "epoch:  584  -  cost:  0.007577971  - MSE:  69.1096061435421 - Train Accuracy:  1.0\n",
            "epoch:  585  -  cost:  0.0075332946  - MSE:  69.29163344062547 - Train Accuracy:  1.0\n",
            "epoch:  586  -  cost:  0.0075173066  - MSE:  69.26191540577953 - Train Accuracy:  1.0\n",
            "epoch:  587  -  cost:  0.0075304364  - MSE:  69.48617412598158 - Train Accuracy:  1.0\n",
            "epoch:  588  -  cost:  0.0074632284  - MSE:  69.41488151411723 - Train Accuracy:  1.0\n",
            "epoch:  589  -  cost:  0.007432213  - MSE:  69.65253099576135 - Train Accuracy:  1.0\n",
            "epoch:  590  -  cost:  0.0074088243  - MSE:  69.71809501571158 - Train Accuracy:  1.0\n",
            "epoch:  591  -  cost:  0.007415479  - MSE:  69.84042846036357 - Train Accuracy:  1.0\n",
            "epoch:  592  -  cost:  0.007353484  - MSE:  69.77123265868617 - Train Accuracy:  1.0\n",
            "epoch:  593  -  cost:  0.00732098  - MSE:  70.01345816507484 - Train Accuracy:  1.0\n",
            "epoch:  594  -  cost:  0.0073137167  - MSE:  70.01118984536295 - Train Accuracy:  1.0\n",
            "epoch:  595  -  cost:  0.0072959163  - MSE:  70.15061811584135 - Train Accuracy:  1.0\n",
            "epoch:  596  -  cost:  0.0072459574  - MSE:  70.14928884898005 - Train Accuracy:  1.0\n",
            "epoch:  597  -  cost:  0.0072113653  - MSE:  70.38490716525703 - Train Accuracy:  1.0\n",
            "epoch:  598  -  cost:  0.0071984124  - MSE:  70.39398579542507 - Train Accuracy:  1.0\n",
            "epoch:  599  -  cost:  0.0072013987  - MSE:  70.55471318231353 - Train Accuracy:  1.0\n",
            "epoch:  600  -  cost:  0.0071408614  - MSE:  70.51409230430173 - Train Accuracy:  1.0\n",
            "epoch:  601  -  cost:  0.0070972578  - MSE:  70.65744621022556 - Train Accuracy:  1.0\n",
            "epoch:  602  -  cost:  0.007107851  - MSE:  70.66466516255393 - Train Accuracy:  1.0\n",
            "epoch:  603  -  cost:  0.0070767365  - MSE:  70.74875354494986 - Train Accuracy:  1.0\n",
            "epoch:  604  -  cost:  0.0070410203  - MSE:  70.75728337551661 - Train Accuracy:  1.0\n",
            "epoch:  605  -  cost:  0.0069984174  - MSE:  70.99252282433989 - Train Accuracy:  1.0\n",
            "epoch:  606  -  cost:  0.006980236  - MSE:  71.01551274822226 - Train Accuracy:  1.0\n",
            "epoch:  607  -  cost:  0.0069551296  - MSE:  71.21603768974964 - Train Accuracy:  1.0\n",
            "epoch:  608  -  cost:  0.006928741  - MSE:  71.14052374223212 - Train Accuracy:  1.0\n",
            "epoch:  609  -  cost:  0.006898072  - MSE:  71.32656846422627 - Train Accuracy:  1.0\n",
            "epoch:  610  -  cost:  0.006880261  - MSE:  71.32047873912117 - Train Accuracy:  1.0\n",
            "epoch:  611  -  cost:  0.0068837656  - MSE:  71.51745284534064 - Train Accuracy:  1.0\n",
            "epoch:  612  -  cost:  0.006838667  - MSE:  71.45690738931403 - Train Accuracy:  1.0\n",
            "epoch:  613  -  cost:  0.006807483  - MSE:  71.66986822005367 - Train Accuracy:  1.0\n",
            "epoch:  614  -  cost:  0.0067970376  - MSE:  71.73476496638183 - Train Accuracy:  1.0\n",
            "epoch:  615  -  cost:  0.006791958  - MSE:  71.84840527542674 - Train Accuracy:  1.0\n",
            "epoch:  616  -  cost:  0.0067389808  - MSE:  71.83814504180324 - Train Accuracy:  1.0\n",
            "epoch:  617  -  cost:  0.006711673  - MSE:  72.04581230668968 - Train Accuracy:  1.0\n",
            "epoch:  618  -  cost:  0.0067131654  - MSE:  71.99327715946927 - Train Accuracy:  1.0\n",
            "epoch:  619  -  cost:  0.006692515  - MSE:  72.17080873412165 - Train Accuracy:  1.0\n",
            "epoch:  620  -  cost:  0.006649305  - MSE:  72.16516964829655 - Train Accuracy:  1.0\n",
            "epoch:  621  -  cost:  0.006639605  - MSE:  72.2410060755286 - Train Accuracy:  1.0\n",
            "epoch:  622  -  cost:  0.006607061  - MSE:  72.24977811147771 - Train Accuracy:  1.0\n",
            "epoch:  623  -  cost:  0.0065735877  - MSE:  72.4583447753268 - Train Accuracy:  1.0\n",
            "epoch:  624  -  cost:  0.006553227  - MSE:  72.47989280975189 - Train Accuracy:  1.0\n",
            "epoch:  625  -  cost:  0.006530848  - MSE:  72.67509613560226 - Train Accuracy:  1.0\n",
            "epoch:  626  -  cost:  0.0065038046  - MSE:  72.66666312575333 - Train Accuracy:  1.0\n",
            "epoch:  627  -  cost:  0.006481911  - MSE:  72.8072120008698 - Train Accuracy:  1.0\n",
            "epoch:  628  -  cost:  0.00646537  - MSE:  72.79051727994363 - Train Accuracy:  1.0\n",
            "epoch:  629  -  cost:  0.0064706616  - MSE:  72.92996136308018 - Train Accuracy:  1.0\n",
            "epoch:  630  -  cost:  0.006423358  - MSE:  72.93665831816607 - Train Accuracy:  1.0\n",
            "epoch:  631  -  cost:  0.0063987514  - MSE:  73.16167197979671 - Train Accuracy:  1.0\n",
            "epoch:  632  -  cost:  0.006373442  - MSE:  73.18291855212445 - Train Accuracy:  1.0\n",
            "epoch:  633  -  cost:  0.006396845  - MSE:  73.35181924672304 - Train Accuracy:  1.0\n",
            "epoch:  634  -  cost:  0.0063308096  - MSE:  73.33961549388334 - Train Accuracy:  1.0\n",
            "epoch:  635  -  cost:  0.0063134143  - MSE:  73.43967754203901 - Train Accuracy:  1.0\n",
            "epoch:  636  -  cost:  0.006308025  - MSE:  73.46879441455151 - Train Accuracy:  1.0\n",
            "epoch:  637  -  cost:  0.0062988563  - MSE:  73.65038971810037 - Train Accuracy:  1.0\n",
            "epoch:  638  -  cost:  0.0062539587  - MSE:  73.61225201349211 - Train Accuracy:  1.0\n",
            "epoch:  639  -  cost:  0.006225856  - MSE:  73.77213264015624 - Train Accuracy:  1.0\n",
            "epoch:  640  -  cost:  0.006215292  - MSE:  73.84597213770664 - Train Accuracy:  1.0\n",
            "epoch:  641  -  cost:  0.006212371  - MSE:  73.90447891415599 - Train Accuracy:  1.0\n",
            "epoch:  642  -  cost:  0.006168851  - MSE:  73.97010282703452 - Train Accuracy:  1.0\n",
            "epoch:  643  -  cost:  0.006143434  - MSE:  74.08661762995729 - Train Accuracy:  1.0\n",
            "epoch:  644  -  cost:  0.0061281384  - MSE:  74.13711218875085 - Train Accuracy:  1.0\n",
            "epoch:  645  -  cost:  0.0061036455  - MSE:  74.28022999502258 - Train Accuracy:  1.0\n",
            "epoch:  646  -  cost:  0.00607777  - MSE:  74.3779655423566 - Train Accuracy:  1.0\n",
            "epoch:  647  -  cost:  0.006065898  - MSE:  74.42332633135045 - Train Accuracy:  1.0\n",
            "epoch:  648  -  cost:  0.0060679065  - MSE:  74.48473474581655 - Train Accuracy:  1.0\n",
            "epoch:  649  -  cost:  0.006030609  - MSE:  74.53780119068036 - Train Accuracy:  1.0\n",
            "epoch:  650  -  cost:  0.006005763  - MSE:  74.6909329388859 - Train Accuracy:  1.0\n",
            "epoch:  651  -  cost:  0.0060075284  - MSE:  74.69598094848828 - Train Accuracy:  1.0\n",
            "epoch:  652  -  cost:  0.0059970184  - MSE:  74.80637690283832 - Train Accuracy:  1.0\n",
            "epoch:  653  -  cost:  0.0059542805  - MSE:  74.81285024149292 - Train Accuracy:  1.0\n",
            "epoch:  654  -  cost:  0.0059538796  - MSE:  74.97662807618 - Train Accuracy:  1.0\n",
            "epoch:  655  -  cost:  0.00592119  - MSE:  74.90884914575479 - Train Accuracy:  1.0\n",
            "epoch:  656  -  cost:  0.0058976696  - MSE:  75.0852403725153 - Train Accuracy:  1.0\n",
            "epoch:  657  -  cost:  0.005886973  - MSE:  75.15885162088537 - Train Accuracy:  1.0\n",
            "epoch:  658  -  cost:  0.0058893776  - MSE:  75.25817049026904 - Train Accuracy:  1.0\n",
            "epoch:  659  -  cost:  0.005842969  - MSE:  75.21938315150192 - Train Accuracy:  1.0\n",
            "epoch:  660  -  cost:  0.005847477  - MSE:  75.36136281570907 - Train Accuracy:  1.0\n",
            "epoch:  661  -  cost:  0.005805757  - MSE:  75.37116642098961 - Train Accuracy:  1.0\n",
            "epoch:  662  -  cost:  0.0057858806  - MSE:  75.54527664461428 - Train Accuracy:  1.0\n",
            "epoch:  663  -  cost:  0.0057639168  - MSE:  75.55241118081273 - Train Accuracy:  1.0\n",
            "epoch:  664  -  cost:  0.005761167  - MSE:  75.58122045712948 - Train Accuracy:  1.0\n",
            "epoch:  665  -  cost:  0.0057578906  - MSE:  75.65628061218133 - Train Accuracy:  1.0\n",
            "epoch:  666  -  cost:  0.0057196524  - MSE:  75.75255448072856 - Train Accuracy:  1.0\n",
            "epoch:  667  -  cost:  0.0057226494  - MSE:  75.81569776107494 - Train Accuracy:  1.0\n",
            "epoch:  668  -  cost:  0.005684451  - MSE:  75.81989732471845 - Train Accuracy:  1.0\n",
            "epoch:  669  -  cost:  0.005663509  - MSE:  75.9568964525243 - Train Accuracy:  1.0\n",
            "epoch:  670  -  cost:  0.005652338  - MSE:  76.0140865897836 - Train Accuracy:  1.0\n",
            "epoch:  671  -  cost:  0.0056405235  - MSE:  76.11221159695097 - Train Accuracy:  1.0\n",
            "epoch:  672  -  cost:  0.005630879  - MSE:  76.15785143216354 - Train Accuracy:  1.0\n",
            "epoch:  673  -  cost:  0.0056015295  - MSE:  76.23079627481107 - Train Accuracy:  1.0\n",
            "epoch:  674  -  cost:  0.005596939  - MSE:  76.29241782823483 - Train Accuracy:  1.0\n",
            "epoch:  675  -  cost:  0.005568157  - MSE:  76.27057481709491 - Train Accuracy:  1.0\n",
            "epoch:  676  -  cost:  0.005574289  - MSE:  76.36628849754594 - Train Accuracy:  1.0\n",
            "epoch:  677  -  cost:  0.005538638  - MSE:  76.45986269088792 - Train Accuracy:  1.0\n",
            "epoch:  678  -  cost:  0.0055137016  - MSE:  76.51269602791729 - Train Accuracy:  1.0\n",
            "epoch:  679  -  cost:  0.005501025  - MSE:  76.55407112270538 - Train Accuracy:  1.0\n",
            "epoch:  680  -  cost:  0.005482816  - MSE:  76.76495285869186 - Train Accuracy:  1.0\n",
            "epoch:  681  -  cost:  0.005471502  - MSE:  76.795026967635 - Train Accuracy:  1.0\n",
            "epoch:  682  -  cost:  0.0054650283  - MSE:  76.78649427089367 - Train Accuracy:  1.0\n",
            "epoch:  683  -  cost:  0.005453621  - MSE:  76.88880641958193 - Train Accuracy:  1.0\n",
            "epoch:  684  -  cost:  0.005449196  - MSE:  77.07003541186312 - Train Accuracy:  1.0\n",
            "epoch:  685  -  cost:  0.005408151  - MSE:  77.05217821611988 - Train Accuracy:  1.0\n",
            "epoch:  686  -  cost:  0.005403588  - MSE:  77.10397208573514 - Train Accuracy:  1.0\n",
            "epoch:  687  -  cost:  0.0053809616  - MSE:  77.12524242044763 - Train Accuracy:  1.0\n",
            "epoch:  688  -  cost:  0.005384195  - MSE:  77.22784522443288 - Train Accuracy:  1.0\n",
            "epoch:  689  -  cost:  0.0053468896  - MSE:  77.27479514704176 - Train Accuracy:  1.0\n",
            "epoch:  690  -  cost:  0.005326155  - MSE:  77.36454715892168 - Train Accuracy:  1.0\n",
            "epoch:  691  -  cost:  0.005312769  - MSE:  77.39270701995342 - Train Accuracy:  1.0\n",
            "epoch:  692  -  cost:  0.0053157904  - MSE:  77.51961854945527 - Train Accuracy:  1.0\n",
            "epoch:  693  -  cost:  0.0052985228  - MSE:  77.51086630239102 - Train Accuracy:  1.0\n",
            "epoch:  694  -  cost:  0.005293863  - MSE:  77.67193549454753 - Train Accuracy:  1.0\n",
            "epoch:  695  -  cost:  0.005257054  - MSE:  77.68281049072652 - Train Accuracy:  1.0\n",
            "epoch:  696  -  cost:  0.0052380604  - MSE:  77.76581799192469 - Train Accuracy:  1.0\n",
            "epoch:  697  -  cost:  0.005228145  - MSE:  77.83447079442539 - Train Accuracy:  1.0\n",
            "epoch:  698  -  cost:  0.005223209  - MSE:  77.96216663636571 - Train Accuracy:  1.0\n",
            "epoch:  699  -  cost:  0.0052133645  - MSE:  78.03674770961378 - Train Accuracy:  1.0\n",
            "epoch:  700  -  cost:  0.005187968  - MSE:  78.02651789830372 - Train Accuracy:  1.0\n",
            "epoch:  701  -  cost:  0.005193737  - MSE:  78.18652745166614 - Train Accuracy:  1.0\n",
            "epoch:  702  -  cost:  0.0051559345  - MSE:  78.15249057529864 - Train Accuracy:  1.0\n",
            "epoch:  703  -  cost:  0.0051374873  - MSE:  78.36057596124394 - Train Accuracy:  1.0\n",
            "epoch:  704  -  cost:  0.005122228  - MSE:  78.32384309539601 - Train Accuracy:  1.0\n",
            "epoch:  705  -  cost:  0.0051145726  - MSE:  78.42854970291252 - Train Accuracy:  1.0\n",
            "epoch:  706  -  cost:  0.005097674  - MSE:  78.48555382763932 - Train Accuracy:  1.0\n",
            "epoch:  707  -  cost:  0.0051014177  - MSE:  78.55612432078121 - Train Accuracy:  1.0\n",
            "epoch:  708  -  cost:  0.005068828  - MSE:  78.57765034085666 - Train Accuracy:  1.0\n",
            "epoch:  709  -  cost:  0.0050724456  - MSE:  78.71268985312612 - Train Accuracy:  1.0\n",
            "epoch:  710  -  cost:  0.0050422396  - MSE:  78.78615375413338 - Train Accuracy:  1.0\n",
            "epoch:  711  -  cost:  0.0050284863  - MSE:  78.84543161262064 - Train Accuracy:  1.0\n",
            "epoch:  712  -  cost:  0.0050183632  - MSE:  78.98211769135354 - Train Accuracy:  1.0\n",
            "epoch:  713  -  cost:  0.005021367  - MSE:  79.00193463686983 - Train Accuracy:  1.0\n",
            "epoch:  714  -  cost:  0.0049836454  - MSE:  79.0158674441295 - Train Accuracy:  1.0\n",
            "epoch:  715  -  cost:  0.00497042  - MSE:  79.14857242869788 - Train Accuracy:  1.0\n",
            "epoch:  716  -  cost:  0.0049728784  - MSE:  79.15748548776996 - Train Accuracy:  1.0\n",
            "epoch:  717  -  cost:  0.0049598226  - MSE:  79.28806474989563 - Train Accuracy:  1.0\n",
            "epoch:  718  -  cost:  0.004936115  - MSE:  79.33548056760078 - Train Accuracy:  1.0\n",
            "epoch:  719  -  cost:  0.004932443  - MSE:  79.3632689862998 - Train Accuracy:  1.0\n",
            "epoch:  720  -  cost:  0.004908379  - MSE:  79.44754013047627 - Train Accuracy:  1.0\n",
            "epoch:  721  -  cost:  0.0049082213  - MSE:  79.50926492340832 - Train Accuracy:  1.0\n",
            "epoch:  722  -  cost:  0.004880444  - MSE:  79.58793959806573 - Train Accuracy:  1.0\n",
            "epoch:  723  -  cost:  0.0048616715  - MSE:  79.66589916160027 - Train Accuracy:  1.0\n",
            "epoch:  724  -  cost:  0.0048548756  - MSE:  79.82634941162571 - Train Accuracy:  1.0\n",
            "epoch:  725  -  cost:  0.00483975  - MSE:  79.77942970257382 - Train Accuracy:  1.0\n",
            "epoch:  726  -  cost:  0.004839302  - MSE:  79.90164183344018 - Train Accuracy:  1.0\n",
            "epoch:  727  -  cost:  0.0048268544  - MSE:  79.91344120031823 - Train Accuracy:  1.0\n",
            "epoch:  728  -  cost:  0.0048059267  - MSE:  79.99114879946326 - Train Accuracy:  1.0\n",
            "epoch:  729  -  cost:  0.0048017898  - MSE:  80.00651381100836 - Train Accuracy:  1.0\n",
            "epoch:  730  -  cost:  0.004779098  - MSE:  80.0792886430475 - Train Accuracy:  1.0\n",
            "epoch:  731  -  cost:  0.004760876  - MSE:  80.18760237631281 - Train Accuracy:  1.0\n",
            "epoch:  732  -  cost:  0.00475772  - MSE:  80.36445983633506 - Train Accuracy:  1.0\n",
            "epoch:  733  -  cost:  0.0047499402  - MSE:  80.3318802022885 - Train Accuracy:  1.0\n",
            "epoch:  734  -  cost:  0.0047283713  - MSE:  80.37022242321235 - Train Accuracy:  1.0\n",
            "epoch:  735  -  cost:  0.0047259848  - MSE:  80.43918212698603 - Train Accuracy:  1.0\n",
            "epoch:  736  -  cost:  0.004701673  - MSE:  80.53970240117364 - Train Accuracy:  1.0\n",
            "epoch:  737  -  cost:  0.0046943743  - MSE:  80.58278637108991 - Train Accuracy:  1.0\n",
            "epoch:  738  -  cost:  0.0046828077  - MSE:  80.71107387911127 - Train Accuracy:  1.0\n",
            "epoch:  739  -  cost:  0.004675334  - MSE:  80.68238753399653 - Train Accuracy:  1.0\n",
            "epoch:  740  -  cost:  0.004655069  - MSE:  80.80043653021667 - Train Accuracy:  1.0\n",
            "epoch:  741  -  cost:  0.0046412935  - MSE:  80.84043069924581 - Train Accuracy:  1.0\n",
            "epoch:  742  -  cost:  0.0046335654  - MSE:  80.94442840452678 - Train Accuracy:  1.0\n",
            "epoch:  743  -  cost:  0.004622328  - MSE:  80.96937922202235 - Train Accuracy:  1.0\n",
            "epoch:  744  -  cost:  0.0046047242  - MSE:  81.08943614751747 - Train Accuracy:  1.0\n",
            "epoch:  745  -  cost:  0.0045930604  - MSE:  81.09808482219891 - Train Accuracy:  1.0\n",
            "epoch:  746  -  cost:  0.0045822184  - MSE:  81.2476503700462 - Train Accuracy:  1.0\n",
            "epoch:  747  -  cost:  0.004583038  - MSE:  81.25019428520132 - Train Accuracy:  1.0\n",
            "epoch:  748  -  cost:  0.004558672  - MSE:  81.35969757444663 - Train Accuracy:  1.0\n",
            "epoch:  749  -  cost:  0.004556146  - MSE:  81.35391646496943 - Train Accuracy:  1.0\n",
            "epoch:  750  -  cost:  0.0045378245  - MSE:  81.46956597410743 - Train Accuracy:  1.0\n",
            "epoch:  751  -  cost:  0.0045349775  - MSE:  81.45505312707735 - Train Accuracy:  1.0\n",
            "epoch:  752  -  cost:  0.0045125154  - MSE:  81.5773087438339 - Train Accuracy:  1.0\n",
            "epoch:  753  -  cost:  0.0045030494  - MSE:  81.61655380299382 - Train Accuracy:  1.0\n",
            "epoch:  754  -  cost:  0.0045004506  - MSE:  81.74472822124972 - Train Accuracy:  1.0\n",
            "epoch:  755  -  cost:  0.0044922256  - MSE:  81.76713871989655 - Train Accuracy:  1.0\n",
            "epoch:  756  -  cost:  0.0044700955  - MSE:  81.85878997434092 - Train Accuracy:  1.0\n",
            "epoch:  757  -  cost:  0.004473732  - MSE:  81.94443737030778 - Train Accuracy:  1.0\n",
            "epoch:  758  -  cost:  0.004446012  - MSE:  81.92434233300091 - Train Accuracy:  1.0\n",
            "epoch:  759  -  cost:  0.0044340384  - MSE:  82.1035154314279 - Train Accuracy:  1.0\n",
            "epoch:  760  -  cost:  0.0044200155  - MSE:  82.08831481580889 - Train Accuracy:  1.0\n",
            "epoch:  761  -  cost:  0.00441573  - MSE:  82.23479021732597 - Train Accuracy:  1.0\n",
            "epoch:  762  -  cost:  0.004412881  - MSE:  82.24517590961489 - Train Accuracy:  1.0\n",
            "epoch:  763  -  cost:  0.0043935333  - MSE:  82.36038925949214 - Train Accuracy:  1.0\n",
            "epoch:  764  -  cost:  0.004391769  - MSE:  82.33372013922812 - Train Accuracy:  1.0\n",
            "epoch:  765  -  cost:  0.004370767  - MSE:  82.45167810770693 - Train Accuracy:  1.0\n",
            "epoch:  766  -  cost:  0.004369395  - MSE:  82.45660357226329 - Train Accuracy:  1.0\n",
            "epoch:  767  -  cost:  0.0043482077  - MSE:  82.57579417765166 - Train Accuracy:  1.0\n",
            "epoch:  768  -  cost:  0.0043479986  - MSE:  82.58545387853351 - Train Accuracy:  1.0\n",
            "epoch:  769  -  cost:  0.0043273526  - MSE:  82.69446488629521 - Train Accuracy:  1.0\n",
            "epoch:  770  -  cost:  0.004328818  - MSE:  82.70546390894955 - Train Accuracy:  1.0\n",
            "epoch:  771  -  cost:  0.004305792  - MSE:  82.82174886559122 - Train Accuracy:  1.0\n",
            "epoch:  772  -  cost:  0.004296164  - MSE:  82.87031711425185 - Train Accuracy:  1.0\n",
            "epoch:  773  -  cost:  0.0042951563  - MSE:  82.94276246572402 - Train Accuracy:  1.0\n",
            "epoch:  774  -  cost:  0.004288212  - MSE:  82.98238298679502 - Train Accuracy:  1.0\n",
            "epoch:  775  -  cost:  0.004267321  - MSE:  83.06404809955384 - Train Accuracy:  1.0\n",
            "epoch:  776  -  cost:  0.004269098  - MSE:  83.11357015566215 - Train Accuracy:  1.0\n",
            "epoch:  777  -  cost:  0.004245436  - MSE:  83.16815924344813 - Train Accuracy:  1.0\n",
            "epoch:  778  -  cost:  0.0042403634  - MSE:  83.21181606741204 - Train Accuracy:  1.0\n",
            "epoch:  779  -  cost:  0.0042287726  - MSE:  83.29336496259354 - Train Accuracy:  1.0\n",
            "epoch:  780  -  cost:  0.004228975  - MSE:  83.33083667039337 - Train Accuracy:  1.0\n",
            "epoch:  781  -  cost:  0.004205106  - MSE:  83.39960546658986 - Train Accuracy:  1.0\n",
            "epoch:  782  -  cost:  0.0041933456  - MSE:  83.47022346034481 - Train Accuracy:  1.0\n",
            "epoch:  783  -  cost:  0.0041832402  - MSE:  83.62295695885145 - Train Accuracy:  1.0\n",
            "epoch:  784  -  cost:  0.004178257  - MSE:  83.56248332176675 - Train Accuracy:  1.0\n",
            "epoch:  785  -  cost:  0.004172212  - MSE:  83.68063394579444 - Train Accuracy:  1.0\n",
            "epoch:  786  -  cost:  0.0041641076  - MSE:  83.7221477189996 - Train Accuracy:  1.0\n",
            "epoch:  787  -  cost:  0.0041493843  - MSE:  83.85350089813667 - Train Accuracy:  1.0\n",
            "epoch:  788  -  cost:  0.0041454867  - MSE:  83.83892612197319 - Train Accuracy:  1.0\n",
            "epoch:  789  -  cost:  0.0041292934  - MSE:  83.94569134445348 - Train Accuracy:  1.0\n",
            "epoch:  790  -  cost:  0.0041256  - MSE:  83.94987443003572 - Train Accuracy:  1.0\n",
            "epoch:  791  -  cost:  0.004109286  - MSE:  84.05892108110413 - Train Accuracy:  1.0\n",
            "epoch:  792  -  cost:  0.0041084345  - MSE:  84.06062938296822 - Train Accuracy:  1.0\n",
            "epoch:  793  -  cost:  0.004089571  - MSE:  84.16891117114912 - Train Accuracy:  1.0\n",
            "epoch:  794  -  cost:  0.0040766587  - MSE:  84.23705689722446 - Train Accuracy:  1.0\n",
            "epoch:  795  -  cost:  0.004067297  - MSE:  84.27131220978646 - Train Accuracy:  1.0\n",
            "epoch:  796  -  cost:  0.004062436  - MSE:  84.37272270096898 - Train Accuracy:  1.0\n",
            "epoch:  797  -  cost:  0.0040539727  - MSE:  84.44286465175406 - Train Accuracy:  1.0\n",
            "epoch:  798  -  cost:  0.0040542595  - MSE:  84.46266715234424 - Train Accuracy:  1.0\n",
            "epoch:  799  -  cost:  0.00403278  - MSE:  84.50850774310398 - Train Accuracy:  1.0\n",
            "epoch:  800  -  cost:  0.0040326803  - MSE:  84.5823135880534 - Train Accuracy:  1.0\n",
            "epoch:  801  -  cost:  0.00401685  - MSE:  84.65608778543599 - Train Accuracy:  1.0\n",
            "epoch:  802  -  cost:  0.004015111  - MSE:  84.68660961601915 - Train Accuracy:  1.0\n",
            "epoch:  803  -  cost:  0.003997391  - MSE:  84.76170101139785 - Train Accuracy:  1.0\n",
            "epoch:  804  -  cost:  0.0039959336  - MSE:  84.78819722565096 - Train Accuracy:  1.0\n",
            "epoch:  805  -  cost:  0.003978704  - MSE:  84.88660640295873 - Train Accuracy:  1.0\n",
            "epoch:  806  -  cost:  0.003970867  - MSE:  84.93227311368298 - Train Accuracy:  1.0\n",
            "epoch:  807  -  cost:  0.0039616358  - MSE:  85.02415826366165 - Train Accuracy:  1.0\n",
            "epoch:  808  -  cost:  0.003958189  - MSE:  85.07450559561325 - Train Accuracy:  1.0\n",
            "epoch:  809  -  cost:  0.0039468273  - MSE:  85.17473877969171 - Train Accuracy:  1.0\n",
            "epoch:  810  -  cost:  0.0039425045  - MSE:  85.19266920252687 - Train Accuracy:  1.0\n",
            "epoch:  811  -  cost:  0.0039240737  - MSE:  85.23793974962209 - Train Accuracy:  1.0\n",
            "epoch:  812  -  cost:  0.003926357  - MSE:  85.32337380764285 - Train Accuracy:  1.0\n",
            "epoch:  813  -  cost:  0.0039093182  - MSE:  85.31300420218038 - Train Accuracy:  1.0\n",
            "epoch:  814  -  cost:  0.0039108307  - MSE:  85.45390679183686 - Train Accuracy:  1.0\n",
            "epoch:  815  -  cost:  0.0038917942  - MSE:  85.41654129856782 - Train Accuracy:  1.0\n",
            "epoch:  816  -  cost:  0.0038945419  - MSE:  85.55466269619244 - Train Accuracy:  1.0\n",
            "epoch:  817  -  cost:  0.003873331  - MSE:  85.55533480958104 - Train Accuracy:  1.0\n",
            "epoch:  818  -  cost:  0.0038627766  - MSE:  85.67810564467142 - Train Accuracy:  1.0\n",
            "epoch:  819  -  cost:  0.003861439  - MSE:  85.7051905269187 - Train Accuracy:  1.0\n",
            "epoch:  820  -  cost:  0.0038541842  - MSE:  85.81348850680658 - Train Accuracy:  1.0\n",
            "epoch:  821  -  cost:  0.0038428705  - MSE:  85.78758326333352 - Train Accuracy:  1.0\n",
            "epoch:  822  -  cost:  0.0038425669  - MSE:  85.93208270692423 - Train Accuracy:  1.0\n",
            "epoch:  823  -  cost:  0.003828168  - MSE:  85.89498024200361 - Train Accuracy:  1.0\n",
            "epoch:  824  -  cost:  0.0038217823  - MSE:  86.03066369429555 - Train Accuracy:  1.0\n",
            "epoch:  825  -  cost:  0.003808597  - MSE:  86.03100396611065 - Train Accuracy:  1.0\n",
            "epoch:  826  -  cost:  0.0038066362  - MSE:  86.15103280026825 - Train Accuracy:  1.0\n",
            "epoch:  827  -  cost:  0.0037882903  - MSE:  86.19685801155914 - Train Accuracy:  1.0\n",
            "epoch:  828  -  cost:  0.003791299  - MSE:  86.22055318635609 - Train Accuracy:  1.0\n",
            "epoch:  829  -  cost:  0.0037734935  - MSE:  86.2769626550626 - Train Accuracy:  1.0\n",
            "epoch:  830  -  cost:  0.0037741864  - MSE:  86.35646847935813 - Train Accuracy:  1.0\n",
            "epoch:  831  -  cost:  0.0037578365  - MSE:  86.41168916633528 - Train Accuracy:  1.0\n",
            "epoch:  832  -  cost:  0.0037572456  - MSE:  86.43568560761237 - Train Accuracy:  1.0\n",
            "epoch:  833  -  cost:  0.003742505  - MSE:  86.54511455292571 - Train Accuracy:  1.0\n",
            "epoch:  834  -  cost:  0.003732082  - MSE:  86.58960796702917 - Train Accuracy:  1.0\n",
            "epoch:  835  -  cost:  0.003723734  - MSE:  86.66795023346461 - Train Accuracy:  1.0\n",
            "epoch:  836  -  cost:  0.003716837  - MSE:  86.67755351650761 - Train Accuracy:  1.0\n",
            "epoch:  837  -  cost:  0.0037141608  - MSE:  86.76998601547514 - Train Accuracy:  1.0\n",
            "epoch:  838  -  cost:  0.0037077863  - MSE:  86.78263974893581 - Train Accuracy:  1.0\n",
            "epoch:  839  -  cost:  0.003695051  - MSE:  86.87606882649874 - Train Accuracy:  1.0\n",
            "epoch:  840  -  cost:  0.003690371  - MSE:  86.85015506200514 - Train Accuracy:  1.0\n",
            "epoch:  841  -  cost:  0.0036800774  - MSE:  86.9714324898876 - Train Accuracy:  1.0\n",
            "epoch:  842  -  cost:  0.0036764923  - MSE:  87.04531058220716 - Train Accuracy:  1.0\n",
            "epoch:  843  -  cost:  0.0036647953  - MSE:  87.0735465200758 - Train Accuracy:  1.0\n",
            "epoch:  844  -  cost:  0.0036611466  - MSE:  87.1546286124061 - Train Accuracy:  1.0\n",
            "epoch:  845  -  cost:  0.0036509032  - MSE:  87.20482849861698 - Train Accuracy:  1.0\n",
            "epoch:  846  -  cost:  0.0036466843  - MSE:  87.33578197365092 - Train Accuracy:  1.0\n",
            "epoch:  847  -  cost:  0.0036354777  - MSE:  87.27780283596874 - Train Accuracy:  1.0\n",
            "epoch:  848  -  cost:  0.0036292854  - MSE:  87.40604389259695 - Train Accuracy:  1.0\n",
            "epoch:  849  -  cost:  0.0036215507  - MSE:  87.35198283138317 - Train Accuracy:  1.0\n",
            "epoch:  850  -  cost:  0.0036119628  - MSE:  87.49871888033051 - Train Accuracy:  1.0\n",
            "epoch:  851  -  cost:  0.003603923  - MSE:  87.5360940779826 - Train Accuracy:  1.0\n",
            "epoch:  852  -  cost:  0.0035989422  - MSE:  87.62945816059138 - Train Accuracy:  1.0\n",
            "epoch:  853  -  cost:  0.0035908918  - MSE:  87.60508822000457 - Train Accuracy:  1.0\n",
            "epoch:  854  -  cost:  0.0035849838  - MSE:  87.73861530157933 - Train Accuracy:  1.0\n",
            "epoch:  855  -  cost:  0.0035777146  - MSE:  87.73104640654265 - Train Accuracy:  1.0\n",
            "epoch:  856  -  cost:  0.0035704512  - MSE:  87.86669622604548 - Train Accuracy:  1.0\n",
            "epoch:  857  -  cost:  0.0035591638  - MSE:  87.85076016303476 - Train Accuracy:  1.0\n",
            "epoch:  858  -  cost:  0.0035489842  - MSE:  88.01410932743012 - Train Accuracy:  1.0\n",
            "epoch:  859  -  cost:  0.0035448724  - MSE:  87.97174708920872 - Train Accuracy:  1.0\n",
            "epoch:  860  -  cost:  0.003539052  - MSE:  88.04242490528223 - Train Accuracy:  1.0\n",
            "epoch:  861  -  cost:  0.0035312406  - MSE:  88.10410429092708 - Train Accuracy:  1.0\n",
            "epoch:  862  -  cost:  0.0035247416  - MSE:  88.22340767523306 - Train Accuracy:  1.0\n",
            "epoch:  863  -  cost:  0.0035120558  - MSE:  88.16163438041684 - Train Accuracy:  1.0\n",
            "epoch:  864  -  cost:  0.0035037522  - MSE:  88.31209766161402 - Train Accuracy:  1.0\n",
            "epoch:  865  -  cost:  0.0034984264  - MSE:  88.3134266027744 - Train Accuracy:  1.0\n",
            "epoch:  866  -  cost:  0.0034938043  - MSE:  88.43616235797258 - Train Accuracy:  1.0\n",
            "epoch:  867  -  cost:  0.0034824342  - MSE:  88.37764776443001 - Train Accuracy:  1.0\n",
            "epoch:  868  -  cost:  0.0034799217  - MSE:  88.5123030649456 - Train Accuracy:  1.0\n",
            "epoch:  869  -  cost:  0.0034743338  - MSE:  88.51522824155867 - Train Accuracy:  1.0\n",
            "epoch:  870  -  cost:  0.0034664623  - MSE:  88.641555540063 - Train Accuracy:  1.0\n",
            "epoch:  871  -  cost:  0.0034579197  - MSE:  88.59935366926956 - Train Accuracy:  1.0\n",
            "epoch:  872  -  cost:  0.0034496884  - MSE:  88.73131022819958 - Train Accuracy:  1.0\n",
            "epoch:  873  -  cost:  0.00344399  - MSE:  88.71860319629018 - Train Accuracy:  1.0\n",
            "epoch:  874  -  cost:  0.0034342068  - MSE:  88.822437605886 - Train Accuracy:  1.0\n",
            "epoch:  875  -  cost:  0.0034273658  - MSE:  88.89952714814706 - Train Accuracy:  1.0\n",
            "epoch:  876  -  cost:  0.003422176  - MSE:  88.91260073300035 - Train Accuracy:  1.0\n",
            "epoch:  877  -  cost:  0.0034103624  - MSE:  88.96279192848856 - Train Accuracy:  1.0\n",
            "epoch:  878  -  cost:  0.0034099477  - MSE:  88.9806388460094 - Train Accuracy:  1.0\n",
            "epoch:  879  -  cost:  0.0033989672  - MSE:  89.05053382246822 - Train Accuracy:  1.0\n",
            "epoch:  880  -  cost:  0.0033932251  - MSE:  89.07422442677117 - Train Accuracy:  1.0\n",
            "epoch:  881  -  cost:  0.0033852346  - MSE:  89.22268399382975 - Train Accuracy:  1.0\n",
            "epoch:  882  -  cost:  0.0033775012  - MSE:  89.19947353310214 - Train Accuracy:  1.0\n",
            "epoch:  883  -  cost:  0.0033687525  - MSE:  89.30059967982847 - Train Accuracy:  1.0\n",
            "epoch:  884  -  cost:  0.0033651723  - MSE:  89.34296728232046 - Train Accuracy:  1.0\n",
            "epoch:  885  -  cost:  0.003356771  - MSE:  89.4149870670086 - Train Accuracy:  1.0\n",
            "epoch:  886  -  cost:  0.0033523624  - MSE:  89.4616522304871 - Train Accuracy:  1.0\n",
            "epoch:  887  -  cost:  0.003343313  - MSE:  89.5337512283791 - Train Accuracy:  1.0\n",
            "epoch:  888  -  cost:  0.0033367518  - MSE:  89.5152608856754 - Train Accuracy:  1.0\n",
            "epoch:  889  -  cost:  0.0033317946  - MSE:  89.61606996143095 - Train Accuracy:  1.0\n",
            "epoch:  890  -  cost:  0.0033241345  - MSE:  89.66071255722585 - Train Accuracy:  1.0\n",
            "epoch:  891  -  cost:  0.0033179868  - MSE:  89.71970101195865 - Train Accuracy:  1.0\n",
            "epoch:  892  -  cost:  0.0033141342  - MSE:  89.73904173261076 - Train Accuracy:  1.0\n",
            "epoch:  893  -  cost:  0.003302848  - MSE:  89.81778052720628 - Train Accuracy:  1.0\n",
            "epoch:  894  -  cost:  0.0032962523  - MSE:  89.89425036893917 - Train Accuracy:  1.0\n",
            "epoch:  895  -  cost:  0.0032884753  - MSE:  89.92102873124357 - Train Accuracy:  1.0\n",
            "epoch:  896  -  cost:  0.00328283  - MSE:  89.98118322303984 - Train Accuracy:  1.0\n",
            "epoch:  897  -  cost:  0.00327797  - MSE:  90.00163426525098 - Train Accuracy:  1.0\n",
            "epoch:  898  -  cost:  0.0032775456  - MSE:  90.11055456299046 - Train Accuracy:  1.0\n",
            "epoch:  899  -  cost:  0.003261204  - MSE:  90.1416503102582 - Train Accuracy:  1.0\n",
            "epoch:  900  -  cost:  0.0032576763  - MSE:  90.18922899138346 - Train Accuracy:  1.0\n",
            "epoch:  901  -  cost:  0.003248737  - MSE:  90.24536774067779 - Train Accuracy:  1.0\n",
            "epoch:  902  -  cost:  0.0032439954  - MSE:  90.31957170873333 - Train Accuracy:  1.0\n",
            "epoch:  903  -  cost:  0.0032362766  - MSE:  90.30662408816319 - Train Accuracy:  1.0\n",
            "epoch:  904  -  cost:  0.0032304388  - MSE:  90.39274185724355 - Train Accuracy:  1.0\n",
            "epoch:  905  -  cost:  0.0032230853  - MSE:  90.48760892702717 - Train Accuracy:  1.0\n",
            "epoch:  906  -  cost:  0.0032177854  - MSE:  90.48351828136764 - Train Accuracy:  1.0\n",
            "epoch:  907  -  cost:  0.0032098724  - MSE:  90.54694588808715 - Train Accuracy:  1.0\n",
            "epoch:  908  -  cost:  0.003207867  - MSE:  90.62454872163437 - Train Accuracy:  1.0\n",
            "epoch:  909  -  cost:  0.003197286  - MSE:  90.68515468188372 - Train Accuracy:  1.0\n",
            "epoch:  910  -  cost:  0.003190969  - MSE:  90.71821357936366 - Train Accuracy:  1.0\n",
            "epoch:  911  -  cost:  0.0031840925  - MSE:  90.74722586578916 - Train Accuracy:  1.0\n",
            "epoch:  912  -  cost:  0.003179693  - MSE:  90.82280119692034 - Train Accuracy:  1.0\n",
            "epoch:  913  -  cost:  0.0031724717  - MSE:  90.88724205767812 - Train Accuracy:  1.0\n",
            "epoch:  914  -  cost:  0.0031708544  - MSE:  90.90173010192015 - Train Accuracy:  1.0\n",
            "epoch:  915  -  cost:  0.0031583137  - MSE:  90.9875158463976 - Train Accuracy:  1.0\n",
            "epoch:  916  -  cost:  0.0031532221  - MSE:  91.07960127659796 - Train Accuracy:  1.0\n",
            "epoch:  917  -  cost:  0.0031451832  - MSE:  91.06628772979329 - Train Accuracy:  1.0\n",
            "epoch:  918  -  cost:  0.0031417895  - MSE:  91.17957518267531 - Train Accuracy:  1.0\n",
            "epoch:  919  -  cost:  0.0031340483  - MSE:  91.20193481900684 - Train Accuracy:  1.0\n",
            "epoch:  920  -  cost:  0.00312898  - MSE:  91.24281874359318 - Train Accuracy:  1.0\n",
            "epoch:  921  -  cost:  0.0031231511  - MSE:  91.32821162750272 - Train Accuracy:  1.0\n",
            "epoch:  922  -  cost:  0.003122143  - MSE:  91.30916556632754 - Train Accuracy:  1.0\n",
            "epoch:  923  -  cost:  0.0031105292  - MSE:  91.43245928872618 - Train Accuracy:  1.0\n",
            "epoch:  924  -  cost:  0.0031066313  - MSE:  91.44880946301761 - Train Accuracy:  1.0\n",
            "epoch:  925  -  cost:  0.0030996124  - MSE:  91.56227270730282 - Train Accuracy:  1.0\n",
            "epoch:  926  -  cost:  0.0031004385  - MSE:  91.55310268759952 - Train Accuracy:  1.0\n",
            "epoch:  927  -  cost:  0.003086313  - MSE:  91.6457702335541 - Train Accuracy:  1.0\n",
            "epoch:  928  -  cost:  0.003079898  - MSE:  91.62126895269925 - Train Accuracy:  1.0\n",
            "epoch:  929  -  cost:  0.003074861  - MSE:  91.67737586401178 - Train Accuracy:  1.0\n",
            "epoch:  930  -  cost:  0.0030680746  - MSE:  91.80877606169751 - Train Accuracy:  1.0\n",
            "epoch:  931  -  cost:  0.0030641928  - MSE:  91.83578689171534 - Train Accuracy:  1.0\n",
            "epoch:  932  -  cost:  0.0030624983  - MSE:  91.823676031216 - Train Accuracy:  1.0\n",
            "epoch:  933  -  cost:  0.00305171  - MSE:  91.94242866308161 - Train Accuracy:  1.0\n",
            "epoch:  934  -  cost:  0.0030480728  - MSE:  91.95325994769317 - Train Accuracy:  1.0\n",
            "epoch:  935  -  cost:  0.0030394406  - MSE:  92.04132026914304 - Train Accuracy:  1.0\n",
            "epoch:  936  -  cost:  0.003034661  - MSE:  92.04514375177523 - Train Accuracy:  1.0\n",
            "epoch:  937  -  cost:  0.0030293155  - MSE:  92.14523850399932 - Train Accuracy:  1.0\n",
            "epoch:  938  -  cost:  0.0030283588  - MSE:  92.20094614643388 - Train Accuracy:  1.0\n",
            "epoch:  939  -  cost:  0.0030157045  - MSE:  92.26232197864523 - Train Accuracy:  1.0\n",
            "epoch:  940  -  cost:  0.0030136947  - MSE:  92.25976250290687 - Train Accuracy:  1.0\n",
            "epoch:  941  -  cost:  0.0030052254  - MSE:  92.3204572876949 - Train Accuracy:  1.0\n",
            "epoch:  942  -  cost:  0.0030010615  - MSE:  92.39217506919492 - Train Accuracy:  1.0\n",
            "epoch:  943  -  cost:  0.002994349  - MSE:  92.45440105707137 - Train Accuracy:  1.0\n",
            "epoch:  944  -  cost:  0.0029939762  - MSE:  92.47010272284604 - Train Accuracy:  1.0\n",
            "epoch:  945  -  cost:  0.0029817396  - MSE:  92.56622315512296 - Train Accuracy:  1.0\n",
            "epoch:  946  -  cost:  0.0029796918  - MSE:  92.5915012439207 - Train Accuracy:  1.0\n",
            "epoch:  947  -  cost:  0.0029725968  - MSE:  92.68187862218348 - Train Accuracy:  1.0\n",
            "epoch:  948  -  cost:  0.0029718243  - MSE:  92.70831015336155 - Train Accuracy:  1.0\n",
            "epoch:  949  -  cost:  0.0029612617  - MSE:  92.79614436825742 - Train Accuracy:  1.0\n",
            "epoch:  950  -  cost:  0.0029577038  - MSE:  92.76727990927037 - Train Accuracy:  1.0\n",
            "epoch:  951  -  cost:  0.0029497952  - MSE:  92.82640542483594 - Train Accuracy:  1.0\n",
            "epoch:  952  -  cost:  0.0029477011  - MSE:  92.8667356632178 - Train Accuracy:  1.0\n",
            "epoch:  953  -  cost:  0.002938915  - MSE:  92.9427084474848 - Train Accuracy:  1.0\n",
            "epoch:  954  -  cost:  0.0029358855  - MSE:  93.01261879948947 - Train Accuracy:  1.0\n",
            "epoch:  955  -  cost:  0.0029286335  - MSE:  93.03332479103888 - Train Accuracy:  1.0\n",
            "epoch:  956  -  cost:  0.0029277531  - MSE:  93.06598905122752 - Train Accuracy:  1.0\n",
            "epoch:  957  -  cost:  0.0029171824  - MSE:  93.12965598826932 - Train Accuracy:  1.0\n",
            "epoch:  958  -  cost:  0.0029136639  - MSE:  93.16467056656302 - Train Accuracy:  1.0\n",
            "epoch:  959  -  cost:  0.0029082722  - MSE:  93.22605430886772 - Train Accuracy:  1.0\n",
            "epoch:  960  -  cost:  0.00290254  - MSE:  93.27875128099136 - Train Accuracy:  1.0\n",
            "epoch:  961  -  cost:  0.0029025616  - MSE:  93.33596519628693 - Train Accuracy:  1.0\n",
            "epoch:  962  -  cost:  0.0028898562  - MSE:  93.38530816028613 - Train Accuracy:  1.0\n",
            "epoch:  963  -  cost:  0.0028884471  - MSE:  93.44669064876487 - Train Accuracy:  1.0\n",
            "epoch:  964  -  cost:  0.002879796  - MSE:  93.4908911727875 - Train Accuracy:  1.0\n",
            "epoch:  965  -  cost:  0.0028782657  - MSE:  93.4838030460697 - Train Accuracy:  1.0\n",
            "epoch:  966  -  cost:  0.0028696577  - MSE:  93.60310883155441 - Train Accuracy:  1.0\n",
            "epoch:  967  -  cost:  0.0028674183  - MSE:  93.59480118325236 - Train Accuracy:  1.0\n",
            "epoch:  968  -  cost:  0.0028602  - MSE:  93.67329021984372 - Train Accuracy:  1.0\n",
            "epoch:  969  -  cost:  0.0028615475  - MSE:  93.73359927408322 - Train Accuracy:  1.0\n",
            "epoch:  970  -  cost:  0.0028497956  - MSE:  93.81830475274285 - Train Accuracy:  1.0\n",
            "epoch:  971  -  cost:  0.0028436955  - MSE:  93.77004015520362 - Train Accuracy:  1.0\n",
            "epoch:  972  -  cost:  0.002843579  - MSE:  93.86626318223335 - Train Accuracy:  1.0\n",
            "epoch:  973  -  cost:  0.0028410486  - MSE:  93.91814982952495 - Train Accuracy:  1.0\n",
            "epoch:  974  -  cost:  0.0028282295  - MSE:  93.94075421653477 - Train Accuracy:  1.0\n",
            "epoch:  975  -  cost:  0.0028264816  - MSE:  94.00274868758754 - Train Accuracy:  1.0\n",
            "epoch:  976  -  cost:  0.0028197793  - MSE:  94.03347534152599 - Train Accuracy:  1.0\n",
            "epoch:  977  -  cost:  0.0028185465  - MSE:  94.08637895726663 - Train Accuracy:  1.0\n",
            "epoch:  978  -  cost:  0.0028077287  - MSE:  94.18695352609336 - Train Accuracy:  1.0\n",
            "epoch:  979  -  cost:  0.002805046  - MSE:  94.19229089006417 - Train Accuracy:  1.0\n",
            "epoch:  980  -  cost:  0.002799086  - MSE:  94.27429807051067 - Train Accuracy:  1.0\n",
            "epoch:  981  -  cost:  0.002795744  - MSE:  94.29588019257467 - Train Accuracy:  1.0\n",
            "epoch:  982  -  cost:  0.0027905756  - MSE:  94.31062704865872 - Train Accuracy:  1.0\n",
            "epoch:  983  -  cost:  0.0027892338  - MSE:  94.39996387081415 - Train Accuracy:  1.0\n",
            "epoch:  984  -  cost:  0.0027784228  - MSE:  94.42857211626885 - Train Accuracy:  1.0\n",
            "epoch:  985  -  cost:  0.0027748665  - MSE:  94.51974792244852 - Train Accuracy:  1.0\n",
            "epoch:  986  -  cost:  0.0027689107  - MSE:  94.52208504543866 - Train Accuracy:  1.0\n",
            "epoch:  987  -  cost:  0.0027658017  - MSE:  94.60538206274164 - Train Accuracy:  1.0\n",
            "epoch:  988  -  cost:  0.0027618564  - MSE:  94.65654401580508 - Train Accuracy:  1.0\n",
            "epoch:  989  -  cost:  0.0027630497  - MSE:  94.70118248889855 - Train Accuracy:  1.0\n",
            "epoch:  990  -  cost:  0.0027492687  - MSE:  94.72980314289411 - Train Accuracy:  1.0\n",
            "epoch:  991  -  cost:  0.0027465909  - MSE:  94.7835825926897 - Train Accuracy:  1.0\n",
            "epoch:  992  -  cost:  0.0027406306  - MSE:  94.80859015442354 - Train Accuracy:  1.0\n",
            "epoch:  993  -  cost:  0.0027381289  - MSE:  94.832246652708 - Train Accuracy:  1.0\n",
            "epoch:  994  -  cost:  0.0027344532  - MSE:  94.9317286057056 - Train Accuracy:  1.0\n",
            "epoch:  995  -  cost:  0.0027275695  - MSE:  94.9565612239761 - Train Accuracy:  1.0\n",
            "epoch:  996  -  cost:  0.002723732  - MSE:  95.00992412256015 - Train Accuracy:  1.0\n",
            "epoch:  997  -  cost:  0.0027183057  - MSE:  95.05851674744437 - Train Accuracy:  1.0\n",
            "epoch:  998  -  cost:  0.0027165764  - MSE:  95.07028493215019 - Train Accuracy:  1.0\n",
            "epoch:  999  -  cost:  0.0027078271  - MSE:  95.14602866241289 - Train Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3u9Tf7nSuYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "4724bf52-b837-4dba-c427-5d7bed46aafe"
      },
      "source": [
        "#Plot MSE and accuracy graph\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(accuracy_history)\n",
        "plt.show()\n",
        "plt.plot(mse_history)\n",
        "plt.show()\n",
        " \n",
        "#Print the final mean square error\n",
        "#correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "#accuracy = tf.reduce_mean(tf.square(pred_y - test_y))\n",
        "#print(\"Test Accuracy: \", (sess.run(y, feed_dict={x:test_x, y_:test_y} )))\n",
        "#Print the final mean square error\n",
        "#pred_y = sess.run(y, feed_dict={x:test_x})\n",
        "#mse = tf.reduce_mean(tf.square(pred_y- test_y))\n",
        "#print(\"MSE: %.4f\" % sess.run(mse))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwlVIMIfVymc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "050be4eb-b90c-4df3-9215-c88f2d5c9e99"
      },
      "source": [
        "mena_acc=np.mean(accuracy_history)\n",
        "print(mena_acc)\n",
        "mean_mse=np.mean(mse_history)\n",
        "print(mean_mse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9004323\n",
            "54.498492382634836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyr6ozJ-79Bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0dfa61d-23dd-4899-ac81-7987070895a6"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel = 'linear', random_state = 0)\n",
        "classifier.fit(X_train, y_tri)\n",
        "Y_pred_svm = classifier.predict(X_test)\n",
        "Y_pred_svm=Y_pred_svm.astype(float)\n",
        "y_tesi=y_tesi.astype(float)\n",
        "error = np.subtract(y_tesi, Y_pred_svm)\n",
        "\n",
        "error = np.mean(np.abs(error))\n",
        "accuracy = (1 - error) * 100\n",
        "\n",
        "print(\" Accuracy_svm\"  + str(round(accuracy,2)) + \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Accuracy_svm88.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNGvmFfAT3mE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a46e6b59-9e98-43a1-8e0c-ebc7e62afe3d"
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "save_path = saver.save(sess, model_path)\n",
        "print(\"Model saved in file: %s\", save_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved in file: %s /content/sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8sCczjoVxcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "outputId": "ee3d6944-f1d0-47c5-ce49-a9f776e57bdc"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "model_path = \"/content/sample_data\"\n",
        "saver.restore(sess, model_path)\n",
        "\n",
        "prediction = tf.argmax(y, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "print(\"**************************************************************\")\n",
        "print(\"* 1 stands for M, (i.e. Mine) and 0 stands for R (i.e. Rock) *\")\n",
        "print(\"**************************************************************\")\n",
        "for i in range(93,101):\n",
        "    prediction_run = sess.run(prediction, feed_dict={x:X[i].reshape(1,60)})\n",
        "    accuracy_run = sess.run(accuracy, feed_dict={x:X[i].reshape(1,60), y_:Y[i].reshape(1,2)})\n",
        "    print(i,\"Original Class: \", int(sess.run(y_[i][1],feed_dict={y_:Y})), \" Predicted Values: \", prediction_run[0] )\n",
        "    print(\"Accuracy: \",str(accuracy_run*100)+\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/sample_data\n",
            "**************************************************************\n",
            "* 1 stands for M, (i.e. Mine) and 0 stands for R (i.e. Rock) *\n",
            "**************************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 93",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-45f94b114497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"**************************************************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m93\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprediction_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0maccuracy_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Original Class: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Predicted Values: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_run\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 93"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9EEVwiAifrO"
      },
      "source": [
        "W=tf.Variable([0.3],tf.float32)\n",
        "b=tf.Variable([-0.3],tf.float32)\n",
        "x=tf.placeholder(tf.float32)\n",
        "linear_model=W*x+b\n",
        "y=tf.placeholder(tf.float32)\n",
        "square_de=tf.square(linear_model-y)\n",
        "error=tf.reduce_sum(square_de)\n",
        "optimize=tf.train.GradientDescentOptimizer(0.01)\n",
        "train=optimize.minimize(error)\n",
        "init=tf.global_variables_initializer()\n",
        "sess=tf.Session()\n",
        "sess.run(init)\n",
        "for i in range(1000):\n",
        "  (sess.run(train,{x:[1,2,3,4],y:[0,-1,-2,-3]}))\n",
        "print(sess.run([W,b]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7KTPLKp3LpL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}